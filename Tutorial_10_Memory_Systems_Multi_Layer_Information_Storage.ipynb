{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjZXWXsXg/WYl91UwQsxXn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShaliniAnandaPhD/Neuron/blob/main/Tutorial_10_Memory_Systems_Multi_Layer_Information_Storage.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Tutorial 9, you built circuit networks where agents collaborate on tasks.\n",
        "\n",
        " Now we're adding sophisticated memory systems - the ability to store retrieve, and reason with different types of information across time and context.\n",
        "\n",
        "What you'll build:\n",
        "\n",
        "• Multi-layered memory architecture (working, episodic, semantic, procedural, emotional)\n",
        "\n",
        "• Memory consolidation and decay mechanisms\n",
        "\n",
        "• Cross-memory querying and association systems\n",
        "\n",
        "• Memory-aware agents with context retention\n",
        "\n",
        "• Knowledge graph integration for semantic memory\n",
        "\n",
        "• Memory visualization and debugging tools\n",
        "\n",
        "Why this matters:\n",
        "\n",
        "Real intelligence requires memory - not just storing facts, but understanding\n",
        "context, learning from experience, and building knowledge over time. Memory\n",
        "systems enable agents to become truly intelligent by accumulating wisdom\n",
        "and adapting behavior based on past experiences.\n",
        "\n",
        "By the end, you'll understand:\n",
        "\n",
        "• How different memory types serve different cognitive functions\n",
        "\n",
        "• Memory consolidation from working to long-term storage\n",
        "\n",
        "• Cross-memory association and retrieval strategies\n",
        "\n",
        "• Memory-driven decision making and learning\n",
        "\n",
        "• Production memory system architecture and optimization\n",
        "\n"
      ],
      "metadata": {
        "id": "VYc5dVon-wva"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syFL4b3K9dvu",
        "outputId": "f56e85a8-e2b3-4a0f-ef78-8cbdef919d4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tutorial 10: Memory Systems - Multi-Layer Information Storage\n",
            "============================================================\n",
            "\n",
            "Building sophisticated memory architectures for intelligent agents...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Tutorial 10: Memory Systems - Multi-Layer Information Storage\")\n",
        "print(\"=\" * 60)\n",
        "print()\n",
        "print(\"Building sophisticated memory architectures for intelligent agents...\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Essential imports\n",
        "import uuid\n",
        "import time\n",
        "import threading\n",
        "import queue\n",
        "import json\n",
        "import pickle\n",
        "import hashlib\n",
        "import math\n",
        "import heapq\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field, asdict\n",
        "from typing import Any, Dict, List, Optional, Set, Callable, Union, Tuple, Iterator\n",
        "from enum import Enum\n",
        "from collections import defaultdict, deque, OrderedDict, namedtuple\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import sqlite3\n",
        "import weakref\n",
        "from pathlib import Path\n"
      ],
      "metadata": {
        "id": "Q35Nyg5e_KB-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import our foundation from previous tutorials\n",
        "AgentID = str\n",
        "MessageID = str\n",
        "CircuitID = str\n",
        "MemoryID = str\n",
        "\n",
        "class MemoryType(Enum):\n",
        "    \"\"\"Different types of memory systems\"\"\"\n",
        "    WORKING = \"working\"         # Short-term active information\n",
        "    EPISODIC = \"episodic\"      # Personal experiences and events\n",
        "    SEMANTIC = \"semantic\"      # Facts and knowledge\n",
        "    PROCEDURAL = \"procedural\"  # Skills and procedures\n",
        "    EMOTIONAL = \"emotional\"    # Emotional associations and responses\n",
        "\n",
        "class MemoryPriority(Enum):\n",
        "    \"\"\"Memory importance levels\"\"\"\n",
        "    CRITICAL = 5    # Never forget\n",
        "    HIGH = 4        # Important information\n",
        "    NORMAL = 3      # Standard information\n",
        "    LOW = 2         # Background information\n",
        "    TRANSIENT = 1   # Temporary information\n",
        "\n",
        "class ConsolidationStatus(Enum):\n",
        "    \"\"\"Memory consolidation states\"\"\"\n",
        "    FRESH = \"fresh\"                 # Just created\n",
        "    REHEARSED = \"rehearsed\"        # Recently accessed\n",
        "    CONSOLIDATING = \"consolidating\" # Being moved to long-term\n",
        "    STABLE = \"stable\"              # Firmly established\n",
        "    DECAYING = \"decaying\"          # Losing strength\n",
        "    ARCHIVED = \"archived\"          # Moved to cold storage\n",
        "\n",
        "@dataclass\n",
        "class MemoryItem:\n",
        "    \"\"\"\n",
        "    A single memory item with metadata and content\n",
        "\n",
        "    This represents any piece of information stored in memory,\n",
        "    from facts to experiences to learned procedures.\n",
        "    \"\"\"\n",
        "    id: MemoryID\n",
        "    content: Any                                    # The actual memory content\n",
        "    memory_type: MemoryType                        # Which memory system\n",
        "    priority: MemoryPriority = MemoryPriority.NORMAL\n",
        "    created_at: float = field(default_factory=time.time)\n",
        "    last_accessed: float = field(default_factory=time.time)\n",
        "    access_count: int = 0                          # How often accessed\n",
        "    strength: float = 1.0                         # Memory strength (0-1)\n",
        "    consolidation_status: ConsolidationStatus = ConsolidationStatus.FRESH\n",
        "\n",
        "    # Contextual information\n",
        "    context: Dict[str, Any] = field(default_factory=dict)\n",
        "    tags: Set[str] = field(default_factory=set)\n",
        "    associations: Set[MemoryID] = field(default_factory=set)\n",
        "\n",
        "    # Emotional and importance weighting\n",
        "    emotional_valence: float = 0.0                 # -1 (negative) to +1 (positive)\n",
        "    importance_score: float = 0.5                  # 0-1 importance rating\n",
        "\n",
        "    # Source and provenance\n",
        "    source: str = \"unknown\"                        # Where this memory came from\n",
        "    confidence: float = 1.0                        # Confidence in memory accuracy\n",
        "\n",
        "    def access(self):\n",
        "        \"\"\"Record that this memory was accessed\"\"\"\n",
        "        self.last_accessed = time.time()\n",
        "        self.access_count += 1\n",
        "\n",
        "        # Boost strength on access (spaced repetition effect)\n",
        "        strength_boost = 0.1 / (1 + math.log(self.access_count))\n",
        "        self.strength = min(1.0, self.strength + strength_boost)\n",
        "\n",
        "    def decay(self, decay_rate: float = 0.01):\n",
        "        \"\"\"Apply memory decay over time\"\"\"\n",
        "        time_factor = (time.time() - self.last_accessed) / (24 * 3600)  # Days\n",
        "        decay_amount = decay_rate * time_factor * (1 - self.importance_score)\n",
        "        self.strength = max(0.0, self.strength - decay_amount)\n",
        "\n",
        "        # Update consolidation status based on strength\n",
        "        if self.strength < 0.1:\n",
        "            self.consolidation_status = ConsolidationStatus.DECAYING\n",
        "        elif self.strength > 0.8 and self.access_count > 5:\n",
        "            self.consolidation_status = ConsolidationStatus.STABLE\n",
        "\n",
        "    def add_association(self, memory_id: MemoryID):\n",
        "        \"\"\"Add an association to another memory\"\"\"\n",
        "        self.associations.add(memory_id)\n",
        "\n",
        "    def get_age(self) -> float:\n",
        "        \"\"\"Get memory age in days\"\"\"\n",
        "        return (time.time() - self.created_at) / (24 * 3600)\n",
        "\n",
        "    def get_recency(self) -> float:\n",
        "        \"\"\"Get recency of last access in days\"\"\"\n",
        "        return (time.time() - self.last_accessed) / (24 * 3600)\n",
        "\n",
        "    def calculate_retrieval_score(self, query_context: Dict[str, Any] = None) -> float:\n",
        "        \"\"\"Calculate how relevant this memory is for retrieval\"\"\"\n",
        "        # Base score from strength and recency\n",
        "        recency_factor = 1.0 / (1.0 + self.get_recency())\n",
        "        base_score = self.strength * recency_factor * self.importance_score\n",
        "\n",
        "        # Context matching bonus\n",
        "        context_bonus = 0.0\n",
        "        if query_context and self.context:\n",
        "            matching_keys = set(query_context.keys()) & set(self.context.keys())\n",
        "            if matching_keys:\n",
        "                context_bonus = len(matching_keys) / len(self.context)\n",
        "\n",
        "        # Priority bonus\n",
        "        priority_bonus = self.priority.value / 5.0\n",
        "\n",
        "        return base_score + context_bonus * 0.3 + priority_bonus * 0.2\n",
        "\n",
        "class MemoryQuery:\n",
        "    \"\"\"\n",
        "    A query for retrieving memories\n",
        "\n",
        "    This defines what we're looking for in memory and how\n",
        "    to rank and filter the results.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 content_query: str = \"\",\n",
        "                 memory_types: List[MemoryType] = None,\n",
        "                 context_filter: Dict[str, Any] = None,\n",
        "                 tags_filter: Set[str] = None,\n",
        "                 min_strength: float = 0.1,\n",
        "                 max_age_days: float = None,\n",
        "                 limit: int = 10):\n",
        "        self.content_query = content_query\n",
        "        self.memory_types = memory_types or list(MemoryType)\n",
        "        self.context_filter = context_filter or {}\n",
        "        self.tags_filter = tags_filter or set()\n",
        "        self.min_strength = min_strength\n",
        "        self.max_age_days = max_age_days\n",
        "        self.limit = limit\n",
        "        self.created_at = time.time()\n",
        "\n",
        "@dataclass\n",
        "class MemoryAssociation:\n",
        "    \"\"\"\n",
        "    A weighted association between memories\n",
        "\n",
        "    This represents how memories are connected and the\n",
        "    strength of those connections for retrieval.\n",
        "    \"\"\"\n",
        "    source_id: MemoryID\n",
        "    target_id: MemoryID\n",
        "    association_type: str = \"related\"               # Type of association\n",
        "    strength: float = 1.0                          # Association strength\n",
        "    created_at: float = field(default_factory=time.time)\n",
        "    context: Dict[str, Any] = field(default_factory=dict)\n",
        "\n",
        "    def decay(self, decay_rate: float = 0.005):\n",
        "        \"\"\"Decay association strength over time\"\"\"\n",
        "        time_factor = (time.time() - self.created_at) / (24 * 3600)\n",
        "        self.strength = max(0.0, self.strength - decay_rate * time_factor)\n",
        "\n",
        "class WorkingMemory:\n",
        "    \"\"\"\n",
        "    Working memory for active processing\n",
        "\n",
        "    This holds information currently being processed or\n",
        "    manipulated, with very limited capacity but fast access.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int = 7):  # Miller's magic number\n",
        "        self.capacity = capacity\n",
        "        self.items: OrderedDict[MemoryID, MemoryItem] = OrderedDict()\n",
        "        self.total_access_count = 0\n",
        "\n",
        "        print(f\"🧠 Working memory initialized (capacity: {capacity})\")\n",
        "\n",
        "    def store(self, content: Any, context: Dict[str, Any] = None,\n",
        "              tags: Set[str] = None) -> MemoryID:\n",
        "        \"\"\"Store an item in working memory\"\"\"\n",
        "        memory_id = str(uuid.uuid4())\n",
        "\n",
        "        memory_item = MemoryItem(\n",
        "            id=memory_id,\n",
        "            content=content,\n",
        "            memory_type=MemoryType.WORKING,\n",
        "            priority=MemoryPriority.HIGH,  # Working memory is high priority\n",
        "            context=context or {},\n",
        "            tags=tags or set(),\n",
        "            source=\"working_memory\"\n",
        "        )\n",
        "\n",
        "        # Add to working memory\n",
        "        self.items[memory_id] = memory_item\n",
        "\n",
        "        # Evict oldest if over capacity\n",
        "        while len(self.items) > self.capacity:\n",
        "            oldest_id, oldest_item = self.items.popitem(last=False)\n",
        "            print(f\"   🔄 Evicted {oldest_id[:8]}... from working memory\")\n",
        "\n",
        "        print(f\"   💭 Stored in working memory: {memory_id[:8]}...\")\n",
        "        return memory_id\n",
        "\n",
        "    def retrieve(self, memory_id: MemoryID) -> Optional[MemoryItem]:\n",
        "        \"\"\"Retrieve an item from working memory\"\"\"\n",
        "        if memory_id in self.items:\n",
        "            item = self.items[memory_id]\n",
        "            item.access()\n",
        "            self.total_access_count += 1\n",
        "\n",
        "            # Move to end (most recently used)\n",
        "            self.items.move_to_end(memory_id)\n",
        "            return item\n",
        "        return None\n",
        "\n",
        "    def search(self, query: MemoryQuery) -> List[MemoryItem]:\n",
        "        \"\"\"Search working memory\"\"\"\n",
        "        results = []\n",
        "\n",
        "        for item in self.items.values():\n",
        "            # Check memory type filter\n",
        "            if item.memory_type not in query.memory_types:\n",
        "                continue\n",
        "\n",
        "            # Check strength filter\n",
        "            if item.strength < query.min_strength:\n",
        "                continue\n",
        "\n",
        "            # Check age filter\n",
        "            if query.max_age_days and item.get_age() > query.max_age_days:\n",
        "                continue\n",
        "\n",
        "            # Check content query\n",
        "            if query.content_query:\n",
        "                content_str = str(item.content).lower()\n",
        "                if query.content_query.lower() not in content_str:\n",
        "                    continue\n",
        "\n",
        "            # Check tags filter\n",
        "            if query.tags_filter and not query.tags_filter.issubset(item.tags):\n",
        "                continue\n",
        "\n",
        "            item.access()\n",
        "            results.append(item)\n",
        "\n",
        "        # Sort by retrieval score\n",
        "        results.sort(key=lambda x: x.calculate_retrieval_score(query.context_filter),\n",
        "                    reverse=True)\n",
        "\n",
        "        return results[:query.limit]\n",
        "\n",
        "    def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get working memory status\"\"\"\n",
        "        return {\n",
        "            'capacity': self.capacity,\n",
        "            'current_items': len(self.items),\n",
        "            'utilization': len(self.items) / self.capacity,\n",
        "            'total_accesses': self.total_access_count,\n",
        "            'items': [\n",
        "                {\n",
        "                    'id': item.id[:8] + '...',\n",
        "                    'type': str(type(item.content).__name__),\n",
        "                    'strength': item.strength,\n",
        "                    'accesses': item.access_count\n",
        "                }\n",
        "                for item in list(self.items.values())[-5:]  # Last 5 items\n",
        "            ]\n",
        "        }\n",
        "\n",
        "class LongTermMemory:\n",
        "    \"\"\"\n",
        "    Long-term memory storage with different specialized systems\n",
        "\n",
        "    This provides persistent storage for different types of memories\n",
        "    with sophisticated retrieval and consolidation mechanisms.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, storage_path: str = \":memory:\"):\n",
        "        self.storage_path = storage_path\n",
        "        self.memories: Dict[MemoryID, MemoryItem] = {}\n",
        "        self.associations: Dict[Tuple[MemoryID, MemoryID], MemoryAssociation] = {}\n",
        "\n",
        "        # Type-specific indices for fast retrieval\n",
        "        self.type_indices: Dict[MemoryType, Set[MemoryID]] = {\n",
        "            mt: set() for mt in MemoryType\n",
        "        }\n",
        "        self.tag_index: Dict[str, Set[MemoryID]] = defaultdict(set)\n",
        "        self.context_index: Dict[str, Dict[str, Set[MemoryID]]] = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "        # Persistence (SQLite for production use)\n",
        "        self.db_connection = None\n",
        "        self._init_database()\n",
        "\n",
        "        # Background maintenance\n",
        "        self._stop_maintenance = threading.Event()\n",
        "        self._maintenance_thread = threading.Thread(\n",
        "            target=self._maintenance_loop,\n",
        "            daemon=True,\n",
        "            name=\"LTM-Maintenance\"\n",
        "        )\n",
        "        self._maintenance_thread.start()\n",
        "\n",
        "        print(f\"🏛️  Long-term memory initialized (storage: {storage_path})\")\n",
        "\n",
        "    def _init_database(self):\n",
        "        \"\"\"Initialize SQLite database for persistence\"\"\"\n",
        "        try:\n",
        "            self.db_connection = sqlite3.connect(\n",
        "                self.storage_path,\n",
        "                check_same_thread=False,\n",
        "                timeout=10.0\n",
        "            )\n",
        "\n",
        "            # Create tables\n",
        "            self.db_connection.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS memories (\n",
        "                    id TEXT PRIMARY KEY,\n",
        "                    content BLOB,\n",
        "                    memory_type TEXT,\n",
        "                    priority INTEGER,\n",
        "                    created_at REAL,\n",
        "                    last_accessed REAL,\n",
        "                    access_count INTEGER,\n",
        "                    strength REAL,\n",
        "                    consolidation_status TEXT,\n",
        "                    context TEXT,\n",
        "                    tags TEXT,\n",
        "                    emotional_valence REAL,\n",
        "                    importance_score REAL,\n",
        "                    source TEXT,\n",
        "                    confidence REAL\n",
        "                )\n",
        "            \"\"\")\n",
        "\n",
        "            self.db_connection.execute(\"\"\"\n",
        "                CREATE TABLE IF NOT EXISTS associations (\n",
        "                    source_id TEXT,\n",
        "                    target_id TEXT,\n",
        "                    association_type TEXT,\n",
        "                    strength REAL,\n",
        "                    created_at REAL,\n",
        "                    context TEXT,\n",
        "                    PRIMARY KEY (source_id, target_id)\n",
        "                )\n",
        "            \"\"\")\n",
        "\n",
        "            # Create indices\n",
        "            self.db_connection.execute(\"CREATE INDEX IF NOT EXISTS idx_memory_type ON memories(memory_type)\")\n",
        "            self.db_connection.execute(\"CREATE INDEX IF NOT EXISTS idx_strength ON memories(strength)\")\n",
        "            self.db_connection.execute(\"CREATE INDEX IF NOT EXISTS idx_last_accessed ON memories(last_accessed)\")\n",
        "\n",
        "            self.db_connection.commit()\n",
        "\n",
        "            # Load existing memories\n",
        "            self._load_from_database()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Database initialization failed: {e}\")\n",
        "            self.db_connection = None\n",
        "\n",
        "    def _load_from_database(self):\n",
        "        \"\"\"Load memories from database\"\"\"\n",
        "        if not self.db_connection:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            cursor = self.db_connection.execute(\"SELECT * FROM memories\")\n",
        "            for row in cursor:\n",
        "                memory_item = self._row_to_memory_item(row)\n",
        "                self.memories[memory_item.id] = memory_item\n",
        "                self._update_indices(memory_item)\n",
        "\n",
        "            # Load associations\n",
        "            cursor = self.db_connection.execute(\"SELECT * FROM associations\")\n",
        "            for row in cursor:\n",
        "                association = self._row_to_association(row)\n",
        "                key = (association.source_id, association.target_id)\n",
        "                self.associations[key] = association\n",
        "\n",
        "            print(f\"   📚 Loaded {len(self.memories)} memories from database\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Failed to load from database: {e}\")\n",
        "\n",
        "    def _row_to_memory_item(self, row) -> MemoryItem:\n",
        "        \"\"\"Convert database row to MemoryItem\"\"\"\n",
        "        content = pickle.loads(row[1]) if row[1] else None\n",
        "        context = json.loads(row[8]) if row[8] else {}\n",
        "        tags = set(json.loads(row[9])) if row[9] else set()\n",
        "\n",
        "        return MemoryItem(\n",
        "            id=row[0],\n",
        "            content=content,\n",
        "            memory_type=MemoryType(row[2]),\n",
        "            priority=MemoryPriority(row[3]),\n",
        "            created_at=row[4],\n",
        "            last_accessed=row[5],\n",
        "            access_count=row[6],\n",
        "            strength=row[7],\n",
        "            consolidation_status=ConsolidationStatus(row[10]),\n",
        "            context=context,\n",
        "            tags=tags,\n",
        "            emotional_valence=row[11],\n",
        "            importance_score=row[12],\n",
        "            source=row[13],\n",
        "            confidence=row[14]\n",
        "        )\n",
        "\n",
        "    def _row_to_association(self, row) -> MemoryAssociation:\n",
        "        \"\"\"Convert database row to MemoryAssociation\"\"\"\n",
        "        context = json.loads(row[5]) if row[5] else {}\n",
        "\n",
        "        return MemoryAssociation(\n",
        "            source_id=row[0],\n",
        "            target_id=row[1],\n",
        "            association_type=row[2],\n",
        "            strength=row[3],\n",
        "            created_at=row[4],\n",
        "            context=context\n",
        "        )\n",
        "\n",
        "    def store(self, content: Any, memory_type: MemoryType = MemoryType.SEMANTIC,\n",
        "              priority: MemoryPriority = MemoryPriority.NORMAL,\n",
        "              context: Dict[str, Any] = None, tags: Set[str] = None,\n",
        "              emotional_valence: float = 0.0, importance_score: float = 0.5,\n",
        "              source: str = \"ltm\") -> MemoryID:\n",
        "        \"\"\"Store a memory in long-term memory\"\"\"\n",
        "        memory_id = str(uuid.uuid4())\n",
        "\n",
        "        memory_item = MemoryItem(\n",
        "            id=memory_id,\n",
        "            content=content,\n",
        "            memory_type=memory_type,\n",
        "            priority=priority,\n",
        "            context=context or {},\n",
        "            tags=tags or set(),\n",
        "            emotional_valence=emotional_valence,\n",
        "            importance_score=importance_score,\n",
        "            source=source\n",
        "        )\n",
        "\n",
        "        # Store in memory\n",
        "        self.memories[memory_id] = memory_item\n",
        "        self._update_indices(memory_item)\n",
        "\n",
        "        # Persist to database\n",
        "        self._persist_memory(memory_item)\n",
        "\n",
        "        print(f\"   🏛️  Stored in long-term memory: {memory_id[:8]}... ({memory_type.value})\")\n",
        "        return memory_id\n",
        "\n",
        "    def retrieve(self, memory_id: MemoryID) -> Optional[MemoryItem]:\n",
        "        \"\"\"Retrieve a specific memory\"\"\"\n",
        "        if memory_id in self.memories:\n",
        "            item = self.memories[memory_id]\n",
        "            item.access()\n",
        "            self._persist_memory(item)  # Update access info\n",
        "            return item\n",
        "        return None\n",
        "\n",
        "    def search(self, query: MemoryQuery) -> List[MemoryItem]:\n",
        "        \"\"\"Search long-term memory\"\"\"\n",
        "        candidate_ids = set()\n",
        "\n",
        "        # Use indices for efficient filtering\n",
        "        if query.memory_types:\n",
        "            for memory_type in query.memory_types:\n",
        "                candidate_ids.update(self.type_indices[memory_type])\n",
        "        else:\n",
        "            candidate_ids = set(self.memories.keys())\n",
        "\n",
        "        # Filter by tags if specified\n",
        "        if query.tags_filter:\n",
        "            tag_candidates = set()\n",
        "            for tag in query.tags_filter:\n",
        "                tag_candidates.update(self.tag_index[tag])\n",
        "            candidate_ids &= tag_candidates\n",
        "\n",
        "        # Filter by context if specified\n",
        "        if query.context_filter:\n",
        "            context_candidates = set()\n",
        "            for key, value in query.context_filter.items():\n",
        "                context_candidates.update(self.context_index[key][str(value)])\n",
        "            candidate_ids &= context_candidates\n",
        "\n",
        "        # Apply filters and scoring\n",
        "        results = []\n",
        "        for memory_id in candidate_ids:\n",
        "            item = self.memories[memory_id]\n",
        "\n",
        "            # Apply filters\n",
        "            if item.strength < query.min_strength:\n",
        "                continue\n",
        "\n",
        "            if query.max_age_days and item.get_age() > query.max_age_days:\n",
        "                continue\n",
        "\n",
        "            # Content search\n",
        "            if query.content_query:\n",
        "                content_str = str(item.content).lower()\n",
        "                if query.content_query.lower() not in content_str:\n",
        "                    continue\n",
        "\n",
        "            item.access()\n",
        "            results.append(item)\n",
        "\n",
        "        # Sort by retrieval score\n",
        "        results.sort(key=lambda x: x.calculate_retrieval_score(query.context_filter),\n",
        "                    reverse=True)\n",
        "\n",
        "        return results[:query.limit]\n",
        "\n",
        "    def add_association(self, source_id: MemoryID, target_id: MemoryID,\n",
        "                       association_type: str = \"related\", strength: float = 1.0,\n",
        "                       context: Dict[str, Any] = None) -> bool:\n",
        "        \"\"\"Add an association between memories\"\"\"\n",
        "        if source_id not in self.memories or target_id not in self.memories:\n",
        "            return False\n",
        "\n",
        "        association = MemoryAssociation(\n",
        "            source_id=source_id,\n",
        "            target_id=target_id,\n",
        "            association_type=association_type,\n",
        "            strength=strength,\n",
        "            context=context or {}\n",
        "        )\n",
        "\n",
        "        key = (source_id, target_id)\n",
        "        self.associations[key] = association\n",
        "\n",
        "        # Update memory associations\n",
        "        self.memories[source_id].add_association(target_id)\n",
        "        self.memories[target_id].add_association(source_id)\n",
        "\n",
        "        # Persist association\n",
        "        self._persist_association(association)\n",
        "\n",
        "        return True\n",
        "\n",
        "    def get_associations(self, memory_id: MemoryID,\n",
        "                        association_type: str = None) -> List[MemoryAssociation]:\n",
        "        \"\"\"Get associations for a memory\"\"\"\n",
        "        associations = []\n",
        "\n",
        "        for (source_id, target_id), association in self.associations.items():\n",
        "            if source_id == memory_id or target_id == memory_id:\n",
        "                if association_type is None or association.association_type == association_type:\n",
        "                    associations.append(association)\n",
        "\n",
        "        # Sort by strength\n",
        "        associations.sort(key=lambda x: x.strength, reverse=True)\n",
        "        return associations\n",
        "\n",
        "    def _update_indices(self, memory_item: MemoryItem):\n",
        "        \"\"\"Update search indices\"\"\"\n",
        "        # Type index\n",
        "        self.type_indices[memory_item.memory_type].add(memory_item.id)\n",
        "\n",
        "        # Tag index\n",
        "        for tag in memory_item.tags:\n",
        "            self.tag_index[tag].add(memory_item.id)\n",
        "\n",
        "        # Context index\n",
        "        for key, value in memory_item.context.items():\n",
        "            self.context_index[key][str(value)].add(memory_item.id)\n",
        "\n",
        "    def _persist_memory(self, memory_item: MemoryItem):\n",
        "        \"\"\"Persist memory to database\"\"\"\n",
        "        if not self.db_connection:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.db_connection.execute(\"\"\"\n",
        "                INSERT OR REPLACE INTO memories VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
        "            \"\"\", (\n",
        "                memory_item.id,\n",
        "                pickle.dumps(memory_item.content),\n",
        "                memory_item.memory_type.value,\n",
        "                memory_item.priority.value,\n",
        "                memory_item.created_at,\n",
        "                memory_item.last_accessed,\n",
        "                memory_item.access_count,\n",
        "                memory_item.strength,\n",
        "                memory_item.consolidation_status.value,\n",
        "                json.dumps(memory_item.context),\n",
        "                json.dumps(list(memory_item.tags)),\n",
        "                memory_item.emotional_valence,\n",
        "                memory_item.importance_score,\n",
        "                memory_item.source,\n",
        "                memory_item.confidence\n",
        "            ))\n",
        "            self.db_connection.commit()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Failed to persist memory: {e}\")\n",
        "\n",
        "    def _persist_association(self, association: MemoryAssociation):\n",
        "        \"\"\"Persist association to database\"\"\"\n",
        "        if not self.db_connection:\n",
        "            return\n",
        "\n",
        "        try:\n",
        "            self.db_connection.execute(\"\"\"\n",
        "                INSERT OR REPLACE INTO associations VALUES (?, ?, ?, ?, ?, ?)\n",
        "            \"\"\", (\n",
        "                association.source_id,\n",
        "                association.target_id,\n",
        "                association.association_type,\n",
        "                association.strength,\n",
        "                association.created_at,\n",
        "                json.dumps(association.context)\n",
        "            ))\n",
        "            self.db_connection.commit()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Failed to persist association: {e}\")\n",
        "\n",
        "    def _maintenance_loop(self):\n",
        "        \"\"\"Background maintenance for memory decay and consolidation\"\"\"\n",
        "        while not self._stop_maintenance.wait(60):  # Run every minute\n",
        "            try:\n",
        "                self._apply_decay()\n",
        "                self._consolidate_memories()\n",
        "                self._cleanup_weak_associations()\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Memory maintenance error: {e}\")\n",
        "\n",
        "    def _apply_decay(self):\n",
        "        \"\"\"Apply decay to all memories\"\"\"\n",
        "        for memory_item in self.memories.values():\n",
        "            memory_item.decay()\n",
        "\n",
        "    def _consolidate_memories(self):\n",
        "        \"\"\"Move memories through consolidation stages\"\"\"\n",
        "        for memory_item in self.memories.values():\n",
        "            if (memory_item.consolidation_status == ConsolidationStatus.FRESH and\n",
        "                memory_item.access_count > 2):\n",
        "                memory_item.consolidation_status = ConsolidationStatus.REHEARSED\n",
        "\n",
        "            elif (memory_item.consolidation_status == ConsolidationStatus.REHEARSED and\n",
        "                  memory_item.get_age() > 1.0 and memory_item.access_count > 5):\n",
        "                memory_item.consolidation_status = ConsolidationStatus.CONSOLIDATING\n",
        "\n",
        "            elif (memory_item.consolidation_status == ConsolidationStatus.CONSOLIDATING and\n",
        "                  memory_item.get_age() > 7.0 and memory_item.strength > 0.7):\n",
        "                memory_item.consolidation_status = ConsolidationStatus.STABLE\n",
        "\n",
        "    def _cleanup_weak_associations(self):\n",
        "        \"\"\"Remove very weak associations\"\"\"\n",
        "        to_remove = []\n",
        "        for key, association in self.associations.items():\n",
        "            association.decay()\n",
        "            if association.strength < 0.1:\n",
        "                to_remove.append(key)\n",
        "\n",
        "        for key in to_remove:\n",
        "            del self.associations[key]\n",
        "\n",
        "    def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get long-term memory status\"\"\"\n",
        "        type_counts = {mt.value: len(ids) for mt, ids in self.type_indices.items()}\n",
        "\n",
        "        consolidation_counts = defaultdict(int)\n",
        "        for memory in self.memories.values():\n",
        "            consolidation_counts[memory.consolidation_status.value] += 1\n",
        "\n",
        "        return {\n",
        "            'total_memories': len(self.memories),\n",
        "            'total_associations': len(self.associations),\n",
        "            'by_type': type_counts,\n",
        "            'by_consolidation': dict(consolidation_counts),\n",
        "            'total_tags': len(self.tag_index),\n",
        "            'database_connected': self.db_connection is not None\n",
        "        }\n",
        "\n",
        "    def shutdown(self):\n",
        "        \"\"\"Shutdown long-term memory\"\"\"\n",
        "        self._stop_maintenance.set()\n",
        "        if self.db_connection:\n",
        "            self.db_connection.close()\n",
        "\n",
        "class MemorySystem:\n",
        "    \"\"\"\n",
        "    Integrated memory system combining all memory types\n",
        "\n",
        "    This orchestrates the different memory systems and provides\n",
        "    a unified interface for memory operations and intelligent routing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, working_memory_capacity: int = 7,\n",
        "                 ltm_storage_path: str = \":memory:\"):\n",
        "        self.working_memory = WorkingMemory(working_memory_capacity)\n",
        "        self.long_term_memory = LongTermMemory(ltm_storage_path)\n",
        "\n",
        "        # Memory consolidation queue\n",
        "        self.consolidation_queue = queue.PriorityQueue()\n",
        "        self.consolidation_threshold = 5  # Access count to trigger consolidation\n",
        "\n",
        "        # Background consolidation\n",
        "        self._stop_consolidation = threading.Event()\n",
        "        self._consolidation_thread = threading.Thread(\n",
        "            target=self._consolidation_loop,\n",
        "            daemon=True,\n",
        "            name=\"Memory-Consolidation\"\n",
        "        )\n",
        "        self._consolidation_thread.start()\n",
        "\n",
        "        # Memory statistics\n",
        "        self.consolidation_count = 0\n",
        "        self.retrieval_count = 0\n",
        "        self.total_memories = 0\n",
        "\n",
        "        print(\"🧠 Integrated memory system initialized\")\n",
        "\n",
        "    def store(self, content: Any, memory_type: MemoryType = MemoryType.SEMANTIC,\n",
        "              priority: MemoryPriority = MemoryPriority.NORMAL,\n",
        "              context: Dict[str, Any] = None, tags: Set[str] = None,\n",
        "              emotional_valence: float = 0.0, importance_score: float = 0.5,\n",
        "              force_ltm: bool = False) -> MemoryID:\n",
        "        \"\"\"\n",
        "        Store a memory in the appropriate system\n",
        "\n",
        "        Uses intelligent routing to decide between working memory\n",
        "        and long-term memory based on content and context.\n",
        "        \"\"\"\n",
        "        self.total_memories += 1\n",
        "\n",
        "        # Decide storage location\n",
        "        if (force_ltm or\n",
        "            priority in [MemoryPriority.CRITICAL, MemoryPriority.HIGH] or\n",
        "            memory_type in [MemoryType.SEMANTIC, MemoryType.PROCEDURAL] or\n",
        "            importance_score > 0.7):\n",
        "\n",
        "            # Store directly in long-term memory\n",
        "            memory_id = self.long_term_memory.store(\n",
        "                content=content,\n",
        "                memory_type=memory_type,\n",
        "                priority=priority,\n",
        "                context=context,\n",
        "                tags=tags,\n",
        "                emotional_valence=emotional_valence,\n",
        "                importance_score=importance_score,\n",
        "                source=\"direct_ltm\"\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Store in working memory (might be consolidated later)\n",
        "            memory_id = self.working_memory.store(\n",
        "                content=content,\n",
        "                context=context,\n",
        "                tags=tags\n",
        "            )\n",
        "\n",
        "            # Queue for potential consolidation\n",
        "            if priority != MemoryPriority.TRANSIENT:\n",
        "                consolidation_priority = 10 - priority.value  # Higher priority = lower number\n",
        "                self.consolidation_queue.put((consolidation_priority, time.time(), memory_id))\n",
        "\n",
        "        return memory_id\n",
        "\n",
        "    def retrieve(self, memory_id: MemoryID) -> Optional[MemoryItem]:\n",
        "        \"\"\"Retrieve a memory from any system\"\"\"\n",
        "        self.retrieval_count += 1\n",
        "\n",
        "        # Try working memory first (faster)\n",
        "        memory_item = self.working_memory.retrieve(memory_id)\n",
        "        if memory_item:\n",
        "            return memory_item\n",
        "\n",
        "        # Try long-term memory\n",
        "        memory_item = self.long_term_memory.retrieve(memory_id)\n",
        "        if memory_item:\n",
        "            # Consider promoting to working memory if frequently accessed\n",
        "            if memory_item.access_count > 3 and memory_item.get_recency() < 0.1:\n",
        "                self._promote_to_working_memory(memory_item)\n",
        "\n",
        "        return memory_item\n",
        "\n",
        "    def search(self, query: MemoryQuery) -> List[MemoryItem]:\n",
        "        \"\"\"Search across all memory systems\"\"\"\n",
        "        results = []\n",
        "\n",
        "        # Search working memory\n",
        "        working_results = self.working_memory.search(query)\n",
        "        results.extend(working_results)\n",
        "\n",
        "        # Search long-term memory\n",
        "        ltm_results = self.long_term_memory.search(query)\n",
        "        results.extend(ltm_results)\n",
        "\n",
        "        # Remove duplicates and sort by relevance\n",
        "        seen_ids = set()\n",
        "        unique_results = []\n",
        "        for item in results:\n",
        "            if item.id not in seen_ids:\n",
        "                seen_ids.add(item.id)\n",
        "                unique_results.append(item)\n",
        "\n",
        "        # Sort by retrieval score\n",
        "        unique_results.sort(\n",
        "            key=lambda x: x.calculate_retrieval_score(query.context_filter),\n",
        "            reverse=True\n",
        "        )\n",
        "\n",
        "        return unique_results[:query.limit]\n",
        "\n",
        "    def associate(self, memory_id1: MemoryID, memory_id2: MemoryID,\n",
        "                 association_type: str = \"related\", strength: float = 1.0,\n",
        "                 context: Dict[str, Any] = None) -> bool:\n",
        "        \"\"\"Create an association between memories\"\"\"\n",
        "        # Associations are stored in long-term memory\n",
        "        return self.long_term_memory.add_association(\n",
        "            memory_id1, memory_id2, association_type, strength, context\n",
        "        )\n",
        "\n",
        "    def get_related_memories(self, memory_id: MemoryID,\n",
        "                           association_type: str = None,\n",
        "                           max_results: int = 10) -> List[MemoryItem]:\n",
        "        \"\"\"Get memories related to a given memory\"\"\"\n",
        "        associations = self.long_term_memory.get_associations(memory_id, association_type)\n",
        "\n",
        "        related_memories = []\n",
        "        for association in associations[:max_results]:\n",
        "            # Get the other memory in the association\n",
        "            other_id = (association.target_id if association.source_id == memory_id\n",
        "                       else association.source_id)\n",
        "\n",
        "            other_memory = self.retrieve(other_id)\n",
        "            if other_memory:\n",
        "                related_memories.append(other_memory)\n",
        "\n",
        "        return related_memories\n",
        "\n",
        "    def consolidate_memory(self, memory_id: MemoryID) -> bool:\n",
        "        \"\"\"Manually consolidate a memory from working to long-term\"\"\"\n",
        "        memory_item = self.working_memory.retrieve(memory_id)\n",
        "        if not memory_item:\n",
        "            return False\n",
        "\n",
        "        # Store in long-term memory\n",
        "        ltm_id = self.long_term_memory.store(\n",
        "            content=memory_item.content,\n",
        "            memory_type=MemoryType.EPISODIC,  # Working memories become episodic\n",
        "            priority=memory_item.priority,\n",
        "            context=memory_item.context,\n",
        "            tags=memory_item.tags,\n",
        "            source=\"consolidated_from_wm\"\n",
        "        )\n",
        "\n",
        "        # Remove from working memory\n",
        "        if memory_id in self.working_memory.items:\n",
        "            del self.working_memory.items[memory_id]\n",
        "\n",
        "        self.consolidation_count += 1\n",
        "        print(f\"   🔄 Consolidated memory {memory_id[:8]}... → {ltm_id[:8]}...\")\n",
        "        return True\n",
        "\n",
        "    def _promote_to_working_memory(self, memory_item: MemoryItem):\n",
        "        \"\"\"Promote a frequently accessed LTM item to working memory\"\"\"\n",
        "        # Create a copy in working memory\n",
        "        self.working_memory.store(\n",
        "            content=memory_item.content,\n",
        "            context=memory_item.context,\n",
        "            tags=memory_item.tags\n",
        "        )\n",
        "        print(f\"   ⬆️  Promoted {memory_item.id[:8]}... to working memory\")\n",
        "\n",
        "    def _consolidation_loop(self):\n",
        "        \"\"\"Background consolidation loop\"\"\"\n",
        "        while not self._stop_consolidation.wait(5):  # Check every 5 seconds\n",
        "            try:\n",
        "                # Process consolidation queue\n",
        "                while not self.consolidation_queue.empty():\n",
        "                    try:\n",
        "                        priority, timestamp, memory_id = self.consolidation_queue.get_nowait()\n",
        "\n",
        "                        # Check if memory still exists and meets consolidation criteria\n",
        "                        memory_item = self.working_memory.retrieve(memory_id)\n",
        "                        if (memory_item and\n",
        "                            memory_item.access_count >= self.consolidation_threshold):\n",
        "                            self.consolidate_memory(memory_id)\n",
        "\n",
        "                    except queue.Empty:\n",
        "                        break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"⚠️  Consolidation error: {e}\")\n",
        "\n",
        "    def get_memory_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get comprehensive memory statistics\"\"\"\n",
        "        wm_status = self.working_memory.get_status()\n",
        "        ltm_status = self.long_term_memory.get_status()\n",
        "\n",
        "        return {\n",
        "            'working_memory': wm_status,\n",
        "            'long_term_memory': ltm_status,\n",
        "            'consolidation_count': self.consolidation_count,\n",
        "            'retrieval_count': self.retrieval_count,\n",
        "            'total_memories_created': self.total_memories,\n",
        "            'consolidation_queue_size': self.consolidation_queue.qsize()\n",
        "        }\n",
        "\n",
        "    def shutdown(self):\n",
        "        \"\"\"Shutdown the memory system\"\"\"\n",
        "        self._stop_consolidation.set()\n",
        "        self.long_term_memory.shutdown()\n",
        "        print(\"🛑 Memory system shutdown complete\")\n",
        "\n",
        "class MemoryAwareAgent:\n",
        "    \"\"\"\n",
        "    An agent with sophisticated memory capabilities\n",
        "\n",
        "    This demonstrates how to integrate memory systems into agents\n",
        "    for context-aware processing and learning from experience.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, agent_id: Optional[AgentID] = None, name: str = \"\",\n",
        "                 memory_system: MemorySystem = None,\n",
        "                 memory_capacity: int = 7):\n",
        "        self.id = agent_id or str(uuid.uuid4())\n",
        "        self.name = name or f\"memory_agent_{self.id[:8]}\"\n",
        "\n",
        "        # Memory system\n",
        "        if memory_system:\n",
        "            self.memory = memory_system\n",
        "        else:\n",
        "            self.memory = MemorySystem(\n",
        "                working_memory_capacity=memory_capacity,\n",
        "                ltm_storage_path=f\"agent_{self.id}_memory.db\"\n",
        "            )\n",
        "\n",
        "        # Agent state\n",
        "        self.current_context: Dict[str, Any] = {}\n",
        "        self.conversation_history: List[MemoryID] = []\n",
        "        self.learned_patterns: Dict[str, Any] = {}\n",
        "\n",
        "        # Performance tracking\n",
        "        self.tasks_processed = 0\n",
        "        self.learning_events = 0\n",
        "        self.context_switches = 0\n",
        "\n",
        "        print(f\"🧠 Created memory-aware agent: {self.name}\")\n",
        "\n",
        "    def process_with_memory(self, input_data: Any, context: Dict[str, Any] = None) -> Any:\n",
        "        \"\"\"Process input using memory-enhanced reasoning\"\"\"\n",
        "        self.tasks_processed += 1\n",
        "        processing_context = context or {}\n",
        "\n",
        "        # Store current input in working memory\n",
        "        input_memory_id = self.memory.store(\n",
        "            content=input_data,\n",
        "            memory_type=MemoryType.WORKING,\n",
        "            context=processing_context,\n",
        "            tags={\"input\", \"current\"}\n",
        "        )\n",
        "\n",
        "        # Retrieve relevant memories\n",
        "        relevant_memories = self._get_relevant_context(input_data, processing_context)\n",
        "\n",
        "        # Update current context\n",
        "        self._update_context(processing_context, relevant_memories)\n",
        "\n",
        "        # Process with memory-enhanced reasoning\n",
        "        result = self._memory_enhanced_processing(input_data, relevant_memories)\n",
        "\n",
        "        # Store result and create associations\n",
        "        result_memory_id = self.memory.store(\n",
        "            content=result,\n",
        "            memory_type=MemoryType.EPISODIC,\n",
        "            context=processing_context,\n",
        "            tags={\"output\", \"result\"},\n",
        "            importance_score=0.6\n",
        "        )\n",
        "\n",
        "        # Associate input and output\n",
        "        self.memory.associate(input_memory_id, result_memory_id, \"input_output\")\n",
        "\n",
        "        # Associate with relevant memories\n",
        "        for memory in relevant_memories[:3]:  # Top 3 most relevant\n",
        "            self.memory.associate(result_memory_id, memory.id, \"contextual\")\n",
        "\n",
        "        # Add to conversation history\n",
        "        self.conversation_history.append(result_memory_id)\n",
        "\n",
        "        # Learn from this interaction\n",
        "        self._learn_from_interaction(input_data, result, relevant_memories)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def _get_relevant_context(self, input_data: Any, context: Dict[str, Any]) -> List[MemoryItem]:\n",
        "        \"\"\"Retrieve memories relevant to current input\"\"\"\n",
        "        # Create search query based on input\n",
        "        content_query = str(input_data)[:100]  # First 100 chars\n",
        "\n",
        "        query = MemoryQuery(\n",
        "            content_query=content_query,\n",
        "            memory_types=[MemoryType.EPISODIC, MemoryType.SEMANTIC, MemoryType.PROCEDURAL],\n",
        "            context_filter=context,\n",
        "            min_strength=0.3,\n",
        "            max_age_days=30,  # Focus on recent memories\n",
        "            limit=10\n",
        "        )\n",
        "\n",
        "        return self.memory.search(query)\n",
        "\n",
        "    def _update_context(self, current_context: Dict[str, Any],\n",
        "                       relevant_memories: List[MemoryItem]):\n",
        "        \"\"\"Update agent's current context based on retrieved memories\"\"\"\n",
        "        if self.current_context != current_context:\n",
        "            self.context_switches += 1\n",
        "\n",
        "        self.current_context = current_context.copy()\n",
        "\n",
        "        # Add memory-derived context\n",
        "        for memory in relevant_memories[:5]:\n",
        "            for key, value in memory.context.items():\n",
        "                if key not in self.current_context:\n",
        "                    self.current_context[f\"memory_{key}\"] = value\n",
        "\n",
        "    def _memory_enhanced_processing(self, input_data: Any,\n",
        "                                  relevant_memories: List[MemoryItem]) -> Any:\n",
        "        \"\"\"Process input with memory context\"\"\"\n",
        "        # Basic processing (would be more sophisticated in real implementation)\n",
        "        base_result = f\"Processed: {input_data}\"\n",
        "\n",
        "        # Enhance with memory context\n",
        "        if relevant_memories:\n",
        "            memory_insights = []\n",
        "            for memory in relevant_memories[:3]:\n",
        "                if hasattr(memory.content, '__str__'):\n",
        "                    memory_insights.append(f\"Recalled: {str(memory.content)[:50]}...\")\n",
        "\n",
        "            if memory_insights:\n",
        "                base_result += f\" | Context: {'; '.join(memory_insights)}\"\n",
        "\n",
        "        # Add learned patterns\n",
        "        if self.learned_patterns:\n",
        "            pattern_match = self._find_pattern_match(input_data)\n",
        "            if pattern_match:\n",
        "                base_result += f\" | Pattern: {pattern_match}\"\n",
        "\n",
        "        return base_result\n",
        "\n",
        "    def _learn_from_interaction(self, input_data: Any, result: Any,\n",
        "                              relevant_memories: List[MemoryItem]):\n",
        "        \"\"\"Learn patterns from this interaction\"\"\"\n",
        "        self.learning_events += 1\n",
        "\n",
        "        # Simple pattern learning (extract common elements)\n",
        "        input_str = str(input_data).lower()\n",
        "\n",
        "        # Learn input patterns\n",
        "        input_words = input_str.split()\n",
        "        for word in input_words:\n",
        "            if len(word) > 3:  # Meaningful words\n",
        "                pattern_key = f\"input_pattern_{word}\"\n",
        "                if pattern_key not in self.learned_patterns:\n",
        "                    self.learned_patterns[pattern_key] = {\n",
        "                        'count': 0,\n",
        "                        'contexts': [],\n",
        "                        'results': []\n",
        "                    }\n",
        "\n",
        "                pattern = self.learned_patterns[pattern_key]\n",
        "                pattern['count'] += 1\n",
        "                pattern['contexts'].append(self.current_context.copy())\n",
        "                pattern['results'].append(str(result)[:100])\n",
        "\n",
        "                # Keep only recent examples\n",
        "                if len(pattern['contexts']) > 10:\n",
        "                    pattern['contexts'] = pattern['contexts'][-10:]\n",
        "                    pattern['results'] = pattern['results'][-10:]\n",
        "\n",
        "        # Store learning as semantic memory\n",
        "        if self.learning_events % 10 == 0:  # Every 10 interactions\n",
        "            learning_summary = {\n",
        "                'agent_id': self.id,\n",
        "                'patterns_learned': len(self.learned_patterns),\n",
        "                'interaction_count': self.learning_events,\n",
        "                'recent_patterns': list(self.learned_patterns.keys())[-5:]\n",
        "            }\n",
        "\n",
        "            self.memory.store(\n",
        "                content=learning_summary,\n",
        "                memory_type=MemoryType.SEMANTIC,\n",
        "                priority=MemoryPriority.HIGH,\n",
        "                tags={\"learning\", \"patterns\", \"self_knowledge\"},\n",
        "                importance_score=0.8\n",
        "            )\n",
        "\n",
        "    def _find_pattern_match(self, input_data: Any) -> Optional[str]:\n",
        "        \"\"\"Find matching learned patterns\"\"\"\n",
        "        input_str = str(input_data).lower()\n",
        "\n",
        "        for pattern_key, pattern_data in self.learned_patterns.items():\n",
        "            if pattern_data['count'] > 2:  # Pattern seen multiple times\n",
        "                pattern_word = pattern_key.replace('input_pattern_', '')\n",
        "                if pattern_word in input_str:\n",
        "                    return f\"{pattern_word} (seen {pattern_data['count']} times)\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def recall_conversation(self, limit: int = 10) -> List[MemoryItem]:\n",
        "        \"\"\"Recall recent conversation history\"\"\"\n",
        "        recent_memory_ids = self.conversation_history[-limit:]\n",
        "\n",
        "        memories = []\n",
        "        for memory_id in recent_memory_ids:\n",
        "            memory = self.memory.retrieve(memory_id)\n",
        "            if memory:\n",
        "                memories.append(memory)\n",
        "\n",
        "        return memories\n",
        "\n",
        "    def get_learning_summary(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get summary of what the agent has learned\"\"\"\n",
        "        pattern_summary = {}\n",
        "        for pattern_key, pattern_data in self.learned_patterns.items():\n",
        "            if pattern_data['count'] > 1:  # Only meaningful patterns\n",
        "                pattern_summary[pattern_key] = {\n",
        "                    'frequency': pattern_data['count'],\n",
        "                    'example_result': pattern_data['results'][-1] if pattern_data['results'] else None\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'total_patterns': len(self.learned_patterns),\n",
        "            'meaningful_patterns': len(pattern_summary),\n",
        "            'pattern_details': pattern_summary,\n",
        "            'learning_events': self.learning_events,\n",
        "            'context_switches': self.context_switches,\n",
        "            'conversation_length': len(self.conversation_history)\n",
        "        }\n",
        "\n",
        "    def get_status(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get agent status including memory statistics\"\"\"\n",
        "        memory_stats = self.memory.get_memory_statistics()\n",
        "        learning_summary = self.get_learning_summary()\n",
        "\n",
        "        return {\n",
        "            'id': self.id,\n",
        "            'name': self.name,\n",
        "            'tasks_processed': self.tasks_processed,\n",
        "            'current_context_keys': list(self.current_context.keys()),\n",
        "            'memory_statistics': memory_stats,\n",
        "            'learning_summary': learning_summary\n",
        "        }\n",
        "\n",
        "class KnowledgeGraph:\n",
        "    \"\"\"\n",
        "    Knowledge graph for semantic memory organization\n",
        "\n",
        "    This provides structured knowledge representation with\n",
        "    entities, relationships, and semantic reasoning capabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.entities: Dict[str, Dict[str, Any]] = {}\n",
        "        self.relationships: Dict[str, List[Tuple[str, str, Dict[str, Any]]]] = defaultdict(list)\n",
        "        self.entity_types: Dict[str, str] = {}\n",
        "\n",
        "        # Indices for efficient querying\n",
        "        self.type_index: Dict[str, Set[str]] = defaultdict(set)\n",
        "        self.attribute_index: Dict[str, Dict[str, Set[str]]] = defaultdict(lambda: defaultdict(set))\n",
        "\n",
        "        print(\"🕸️  Knowledge graph initialized\")\n",
        "\n",
        "    def add_entity(self, entity_id: str, entity_type: str,\n",
        "                  attributes: Dict[str, Any] = None) -> bool:\n",
        "        \"\"\"Add an entity to the knowledge graph\"\"\"\n",
        "        if entity_id in self.entities:\n",
        "            return False\n",
        "\n",
        "        self.entities[entity_id] = attributes or {}\n",
        "        self.entity_types[entity_id] = entity_type\n",
        "        self.type_index[entity_type].add(entity_id)\n",
        "\n",
        "        # Update attribute indices\n",
        "        for attr_name, attr_value in (attributes or {}).items():\n",
        "            self.attribute_index[attr_name][str(attr_value)].add(entity_id)\n",
        "\n",
        "        print(f\"   🏷️  Added entity: {entity_id} ({entity_type})\")\n",
        "        return True\n",
        "\n",
        "    def add_relationship(self, source_id: str, relationship_type: str, target_id: str,\n",
        "                        properties: Dict[str, Any] = None) -> bool:\n",
        "        \"\"\"Add a relationship between entities\"\"\"\n",
        "        if source_id not in self.entities or target_id not in self.entities:\n",
        "            return False\n",
        "\n",
        "        relationship = (source_id, target_id, properties or {})\n",
        "        self.relationships[relationship_type].append(relationship)\n",
        "\n",
        "        print(f\"   🔗 Added relationship: {source_id} --{relationship_type}--> {target_id}\")\n",
        "        return True\n",
        "\n",
        "    def query_entities(self, entity_type: str = None,\n",
        "                      attributes: Dict[str, Any] = None) -> List[str]:\n",
        "        \"\"\"Query entities by type and attributes\"\"\"\n",
        "        candidates = set()\n",
        "\n",
        "        if entity_type:\n",
        "            candidates = self.type_index[entity_type].copy()\n",
        "        else:\n",
        "            candidates = set(self.entities.keys())\n",
        "\n",
        "        # Filter by attributes\n",
        "        if attributes:\n",
        "            for attr_name, attr_value in attributes.items():\n",
        "                attr_candidates = self.attribute_index[attr_name][str(attr_value)]\n",
        "                candidates &= attr_candidates\n",
        "\n",
        "        return list(candidates)\n",
        "\n",
        "    def query_relationships(self, relationship_type: str,\n",
        "                          source_id: str = None, target_id: str = None) -> List[Tuple[str, str, Dict[str, Any]]]:\n",
        "        \"\"\"Query relationships\"\"\"\n",
        "        relationships = self.relationships[relationship_type]\n",
        "\n",
        "        if source_id or target_id:\n",
        "            filtered = []\n",
        "            for src, tgt, props in relationships:\n",
        "                if (source_id is None or src == source_id) and \\\n",
        "                   (target_id is None or tgt == target_id):\n",
        "                    filtered.append((src, tgt, props))\n",
        "            return filtered\n",
        "\n",
        "        return relationships\n",
        "\n",
        "    def get_neighbors(self, entity_id: str, relationship_type: str = None) -> List[str]:\n",
        "        \"\"\"Get neighboring entities\"\"\"\n",
        "        neighbors = set()\n",
        "\n",
        "        # Check all relationship types if none specified\n",
        "        rel_types = [relationship_type] if relationship_type else self.relationships.keys()\n",
        "\n",
        "        for rel_type in rel_types:\n",
        "            for src, tgt, props in self.relationships[rel_type]:\n",
        "                if src == entity_id:\n",
        "                    neighbors.add(tgt)\n",
        "                elif tgt == entity_id:\n",
        "                    neighbors.add(src)\n",
        "\n",
        "        return list(neighbors)\n",
        "\n",
        "    def get_path(self, start_id: str, end_id: str, max_depth: int = 3) -> Optional[List[str]]:\n",
        "        \"\"\"Find shortest path between entities\"\"\"\n",
        "        if start_id not in self.entities or end_id not in self.entities:\n",
        "            return None\n",
        "\n",
        "        # BFS to find shortest path\n",
        "        queue = deque([(start_id, [start_id])])\n",
        "        visited = {start_id}\n",
        "\n",
        "        while queue:\n",
        "            current_id, path = queue.popleft()\n",
        "\n",
        "            if len(path) > max_depth:\n",
        "                continue\n",
        "\n",
        "            if current_id == end_id:\n",
        "                return path\n",
        "\n",
        "            for neighbor in self.get_neighbors(current_id):\n",
        "                if neighbor not in visited:\n",
        "                    visited.add(neighbor)\n",
        "                    queue.append((neighbor, path + [neighbor]))\n",
        "\n",
        "        return None\n",
        "\n",
        "    def get_statistics(self) -> Dict[str, Any]:\n",
        "        \"\"\"Get knowledge graph statistics\"\"\"\n",
        "        total_relationships = sum(len(rels) for rels in self.relationships.values())\n",
        "\n",
        "        return {\n",
        "            'total_entities': len(self.entities),\n",
        "            'total_relationships': total_relationships,\n",
        "            'entity_types': len(self.type_index),\n",
        "            'relationship_types': len(self.relationships),\n",
        "            'entity_type_distribution': {\n",
        "                et: len(entities) for et, entities in self.type_index.items()\n",
        "            },\n",
        "            'relationship_type_distribution': {\n",
        "                rt: len(rels) for rt, rels in self.relationships.items()\n",
        "            }\n",
        "        }\n",
        "\n",
        "class MemoryVisualizer:\n",
        "    \"\"\"\n",
        "    Visualization tools for memory systems\n",
        "\n",
        "    This provides debugging and analysis capabilities for\n",
        "    understanding memory system behavior and performance.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, memory_system: MemorySystem):\n",
        "        self.memory_system = memory_system\n",
        "\n",
        "        print(\"📊 Memory visualizer initialized\")\n",
        "\n",
        "    def create_memory_dashboard(self) -> Dict[str, Any]:\n",
        "        \"\"\"Create a comprehensive memory dashboard\"\"\"\n",
        "        stats = self.memory_system.get_memory_statistics()\n",
        "\n",
        "        # Memory distribution analysis\n",
        "        ltm_by_type = stats['long_term_memory']['by_type']\n",
        "        total_ltm = sum(ltm_by_type.values())\n",
        "\n",
        "        type_percentages = {}\n",
        "        for mem_type, count in ltm_by_type.items():\n",
        "            percentage = (count / max(total_ltm, 1)) * 100\n",
        "            type_percentages[mem_type] = percentage\n",
        "\n",
        "        # Consolidation analysis\n",
        "        consolidation_by_status = stats['long_term_memory']['by_consolidation']\n",
        "        total_consolidated = sum(consolidation_by_status.values())\n",
        "\n",
        "        consolidation_percentages = {}\n",
        "        for status, count in consolidation_by_status.items():\n",
        "            percentage = (count / max(total_consolidated, 1)) * 100\n",
        "            consolidation_percentages[status] = percentage\n",
        "\n",
        "        # Working memory efficiency\n",
        "        wm_stats = stats['working_memory']\n",
        "        wm_efficiency = {\n",
        "            'utilization': wm_stats['utilization'] * 100,\n",
        "            'avg_accesses_per_item': wm_stats['total_accesses'] / max(wm_stats['current_items'], 1)\n",
        "        }\n",
        "\n",
        "        return {\n",
        "            'overview': {\n",
        "                'total_memories': stats['total_memories_created'],\n",
        "                'consolidations': stats['consolidation_count'],\n",
        "                'retrievals': stats['retrieval_count'],\n",
        "                'consolidation_rate': stats['consolidation_count'] / max(stats['total_memories_created'], 1)\n",
        "            },\n",
        "            'memory_distribution': {\n",
        "                'by_type_counts': ltm_by_type,\n",
        "                'by_type_percentages': type_percentages\n",
        "            },\n",
        "            'consolidation_analysis': {\n",
        "                'by_status_counts': consolidation_by_status,\n",
        "                'by_status_percentages': consolidation_percentages\n",
        "            },\n",
        "            'working_memory_efficiency': wm_efficiency,\n",
        "            'system_health': {\n",
        "                'consolidation_queue_size': stats['consolidation_queue_size'],\n",
        "                'ltm_database_connected': stats['long_term_memory']['database_connected']\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def analyze_memory_patterns(self, limit: int = 100) -> Dict[str, Any]:\n",
        "        \"\"\"Analyze patterns in memory usage\"\"\"\n",
        "        # Get recent memories for analysis\n",
        "        query = MemoryQuery(\n",
        "            memory_types=list(MemoryType),\n",
        "            limit=limit\n",
        "        )\n",
        "\n",
        "        recent_memories = self.memory_system.search(query)\n",
        "\n",
        "        if not recent_memories:\n",
        "            return {'error': 'No memories found for analysis'}\n",
        "\n",
        "        # Analyze patterns\n",
        "        type_access_patterns = defaultdict(list)\n",
        "        strength_distribution = defaultdict(int)\n",
        "        age_distribution = defaultdict(int)\n",
        "\n",
        "        for memory in recent_memories:\n",
        "            type_access_patterns[memory.memory_type.value].append(memory.access_count)\n",
        "\n",
        "            # Strength distribution (bins)\n",
        "            strength_bin = int(memory.strength * 10) / 10\n",
        "            strength_distribution[strength_bin] += 1\n",
        "\n",
        "            # Age distribution (days)\n",
        "            age_days = int(memory.get_age())\n",
        "            age_distribution[age_days] += 1\n",
        "\n",
        "        # Calculate averages\n",
        "        avg_access_by_type = {}\n",
        "        for mem_type, accesses in type_access_patterns.items():\n",
        "            avg_access_by_type[mem_type] = sum(accesses) / len(accesses)\n",
        "\n",
        "        return {\n",
        "            'sample_size': len(recent_memories),\n",
        "            'average_access_by_type': avg_access_by_type,\n",
        "            'strength_distribution': dict(strength_distribution),\n",
        "            'age_distribution': dict(age_distribution),\n",
        "            'most_accessed_memory': max(recent_memories, key=lambda m: m.access_count),\n",
        "            'strongest_memory': max(recent_memories, key=lambda m: m.strength)\n",
        "        }\n",
        "\n",
        "    def generate_memory_report(self) -> str:\n",
        "        \"\"\"Generate a human-readable memory report\"\"\"\n",
        "        dashboard = self.create_memory_dashboard()\n",
        "        patterns = self.analyze_memory_patterns()\n",
        "\n",
        "        report = []\n",
        "        report.append(\"=\" * 60)\n",
        "        report.append(\"MEMORY SYSTEM ANALYSIS REPORT\")\n",
        "        report.append(\"=\" * 60)\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Overview\n",
        "        overview = dashboard['overview']\n",
        "        report.append(\"📊 SYSTEM OVERVIEW\")\n",
        "        report.append(f\"   Total memories created: {overview['total_memories']}\")\n",
        "        report.append(f\"   Successful consolidations: {overview['consolidations']}\")\n",
        "        report.append(f\"   Total retrievals: {overview['retrievals']}\")\n",
        "        report.append(f\"   Consolidation rate: {overview['consolidation_rate']:.1%}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Memory distribution\n",
        "        distribution = dashboard['memory_distribution']\n",
        "        report.append(\"🧠 MEMORY TYPE DISTRIBUTION\")\n",
        "        for mem_type, percentage in distribution['by_type_percentages'].items():\n",
        "            count = distribution['by_type_counts'][mem_type]\n",
        "            report.append(f\"   {mem_type.capitalize()}: {count} memories ({percentage:.1f}%)\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Working memory efficiency\n",
        "        wm_eff = dashboard['working_memory_efficiency']\n",
        "        report.append(\"💭 WORKING MEMORY EFFICIENCY\")\n",
        "        report.append(f\"   Utilization: {wm_eff['utilization']:.1f}%\")\n",
        "        report.append(f\"   Average accesses per item: {wm_eff['avg_accesses_per_item']:.1f}\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Consolidation analysis\n",
        "        consolidation = dashboard['consolidation_analysis']\n",
        "        report.append(\"🔄 CONSOLIDATION STATUS\")\n",
        "        for status, percentage in consolidation['by_status_percentages'].items():\n",
        "            count = consolidation['by_status_counts'][status]\n",
        "            report.append(f\"   {status.capitalize()}: {count} memories ({percentage:.1f}%)\")\n",
        "        report.append(\"\")\n",
        "\n",
        "        # Pattern analysis\n",
        "        if 'error' not in patterns:\n",
        "            report.append(\"📈 USAGE PATTERNS\")\n",
        "            report.append(f\"   Sample size: {patterns['sample_size']} memories\")\n",
        "            report.append(\"   Average access by type:\")\n",
        "            for mem_type, avg_access in patterns['average_access_by_type'].items():\n",
        "                report.append(f\"     {mem_type.capitalize()}: {avg_access:.1f} accesses\")\n",
        "\n",
        "            most_accessed = patterns['most_accessed_memory']\n",
        "            report.append(f\"   Most accessed memory: {most_accessed.access_count} accesses\")\n",
        "\n",
        "            strongest = patterns['strongest_memory']\n",
        "            report.append(f\"   Strongest memory: {strongest.strength:.2f} strength\")\n",
        "\n",
        "        report.append(\"\")\n",
        "        report.append(\"=\" * 60)\n",
        "\n",
        "        return \"\\n\".join(report)\n",
        "\n",
        "# =============================================================================\n",
        "# INITIALIZATION COMPLETE\n",
        "# =============================================================================\n",
        "\n",
        "print(\"🔧 Tutorial 10 initialization complete!\")\n",
        "print(\"✅ All classes loaded successfully:\")\n",
        "print(\"   - MemoryItem for individual memory storage\")\n",
        "print(\"   - WorkingMemory for short-term active processing\")\n",
        "print(\"   - LongTermMemory for persistent storage with SQLite\")\n",
        "print(\"   - MemorySystem for integrated memory management\")\n",
        "print(\"   - MemoryAwareAgent for intelligent memory-driven processing\")\n",
        "print(\"   - KnowledgeGraph for semantic relationship modeling\")\n",
        "print(\"   - MemoryVisualizer for analysis and debugging\")\n",
        "print()\n",
        "print(\"🚀 Ready to build memory-enhanced intelligent agents!\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05w3hreh_N8S",
        "outputId": "6b4bf29c-62fd-4031-edf1-1c5ee48379f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔧 Tutorial 10 initialization complete!\n",
            "✅ All classes loaded successfully:\n",
            "   - MemoryItem for individual memory storage\n",
            "   - WorkingMemory for short-term active processing\n",
            "   - LongTermMemory for persistent storage with SQLite\n",
            "   - MemorySystem for integrated memory management\n",
            "   - MemoryAwareAgent for intelligent memory-driven processing\n",
            "   - KnowledgeGraph for semantic relationship modeling\n",
            "   - MemoryVisualizer for analysis and debugging\n",
            "\n",
            "🚀 Ready to build memory-enhanced intelligent agents!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DEMO SECTION: Let's build sophisticated memory systems!\n",
        "# =============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"🚀 Tutorial 10: Memory Systems - Multi-Layer Information Storage\")\n",
        "print(\"=\" * 60)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM3OyTP1_WdL",
        "outputId": "26e1b968-25df-4ad6-bf9f-ef3cd0999ffd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "🚀 Tutorial 10: Memory Systems - Multi-Layer Information Storage\n",
            "============================================================\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Create and test basic memory components\n",
        "print(\"📝 Step 1: Creating and testing basic memory components...\")\n",
        "\n",
        "# Create working memory\n",
        "working_memory = WorkingMemory(capacity=5)\n",
        "\n",
        "# Test working memory with different types of content\n",
        "test_data = [\n",
        "    \"Remember this important fact\",\n",
        "    {\"user\": \"Alice\", \"preference\": \"coffee\"},\n",
        "    [\"item1\", \"item2\", \"item3\"],\n",
        "    42,\n",
        "    \"Another piece of information\"\n",
        "]\n",
        "\n",
        "wm_ids = []\n",
        "for i, data in enumerate(test_data):\n",
        "    memory_id = working_memory.store(\n",
        "        content=data,\n",
        "        context={\"session\": \"demo\", \"step\": i},\n",
        "        tags={\"test\", f\"item_{i}\"}\n",
        "    )\n",
        "    wm_ids.append(memory_id)\n",
        "\n",
        "print(f\"   ✅ Stored {len(wm_ids)} items in working memory\")\n",
        "\n",
        "# Test retrieval\n",
        "retrieved = working_memory.retrieve(wm_ids[0])\n",
        "if retrieved:\n",
        "    print(f\"   ✅ Retrieved: {type(retrieved.content).__name__}\")\n",
        "\n",
        "# Test working memory status\n",
        "wm_status = working_memory.get_status()\n",
        "print(f\"   Working memory utilization: {wm_status['utilization']:.1%}\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c018ldfy_Z5p",
        "outputId": "c9e943da-ecf3-4a76-c561-460116bc77fb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 1: Creating and testing basic memory components...\n",
            "🧠 Working memory initialized (capacity: 5)\n",
            "   💭 Stored in working memory: 7311971a...\n",
            "   💭 Stored in working memory: c3b54eed...\n",
            "   💭 Stored in working memory: 266cd44f...\n",
            "   💭 Stored in working memory: 8171ccb3...\n",
            "   💭 Stored in working memory: 1b7c300d...\n",
            "   ✅ Stored 5 items in working memory\n",
            "   ✅ Retrieved: str\n",
            "   Working memory utilization: 100.0%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Create and test long-term memory\n",
        "print(\"📝 Step 2: Creating and testing long-term memory...\")\n",
        "\n",
        "ltm = LongTermMemory(storage_path=\"demo_memory.db\")\n",
        "\n",
        "# Store different types of memories\n",
        "semantic_facts = [\n",
        "    \"Paris is the capital of France\",\n",
        "    \"Python is a programming language\",\n",
        "    \"Machine learning uses neural networks\"\n",
        "]\n",
        "\n",
        "episodic_memories = [\n",
        "    \"Had a meeting with the team at 2 PM\",\n",
        "    \"Learned about memory systems today\",\n",
        "    \"Successfully implemented a new feature\"\n",
        "]\n",
        "\n",
        "procedural_knowledge = [\n",
        "    \"To start a car: 1) Insert key, 2) Turn ignition, 3) Press gas\",\n",
        "    \"To make coffee: 1) Add water, 2) Add grounds, 3) Brew\",\n",
        "    \"To debug code: 1) Read error, 2) Check syntax, 3) Test fix\"\n",
        "]\n",
        "\n",
        "# Store semantic memories\n",
        "semantic_ids = []\n",
        "for fact in semantic_facts:\n",
        "    memory_id = ltm.store(\n",
        "        content=fact,\n",
        "        memory_type=MemoryType.SEMANTIC,\n",
        "        priority=MemoryPriority.HIGH,\n",
        "        context={\"domain\": \"general_knowledge\"},\n",
        "        tags={\"fact\", \"knowledge\"},\n",
        "        importance_score=0.8\n",
        "    )\n",
        "    semantic_ids.append(memory_id)\n",
        "\n",
        "# Store episodic memories\n",
        "episodic_ids = []\n",
        "for episode in episodic_memories:\n",
        "    memory_id = ltm.store(\n",
        "        content=episode,\n",
        "        memory_type=MemoryType.EPISODIC,\n",
        "        priority=MemoryPriority.NORMAL,\n",
        "        context={\"date\": \"2025-05-24\", \"session\": \"demo\"},\n",
        "        tags={\"experience\", \"personal\"},\n",
        "        emotional_valence=0.3,\n",
        "        importance_score=0.6\n",
        "    )\n",
        "    episodic_ids.append(memory_id)\n",
        "\n",
        "# Store procedural knowledge\n",
        "procedural_ids = []\n",
        "for procedure in procedural_knowledge:\n",
        "    memory_id = ltm.store(\n",
        "        content=procedure,\n",
        "        memory_type=MemoryType.PROCEDURAL,\n",
        "        priority=MemoryPriority.HIGH,\n",
        "        context={\"type\": \"instruction\"},\n",
        "        tags={\"procedure\", \"skill\"},\n",
        "        importance_score=0.9\n",
        "    )\n",
        "    procedural_ids.append(memory_id)\n",
        "\n",
        "print(f\"   ✅ Stored {len(semantic_ids)} semantic memories\")\n",
        "print(f\"   ✅ Stored {len(episodic_ids)} episodic memories\")\n",
        "print(f\"   ✅ Stored {len(procedural_ids)} procedural memories\")\n",
        "\n",
        "# Test memory associations\n",
        "ltm.add_association(\n",
        "    semantic_ids[1],  # Python programming\n",
        "    procedural_ids[2],  # Debugging procedure\n",
        "    \"related_skill\",\n",
        "    strength=0.8\n",
        ")\n",
        "\n",
        "print(\"   ✅ Created memory associations\")\n",
        "\n",
        "# Test LTM search\n",
        "search_query = MemoryQuery(\n",
        "    content_query=\"programming\",\n",
        "    memory_types=[MemoryType.SEMANTIC, MemoryType.PROCEDURAL],\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "search_results = ltm.search(search_query)\n",
        "print(f\"   ✅ Search found {len(search_results)} relevant memories\")\n",
        "\n",
        "ltm_status = ltm.get_status()\n",
        "print(f\"   LTM total memories: {ltm_status['total_memories']}\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IISAk99B_eOU",
        "outputId": "9a6e0fe0-7899-42ba-cc96-b63af67fb7dc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 2: Creating and testing long-term memory...\n",
            "   📚 Loaded 0 memories from database\n",
            "🏛️  Long-term memory initialized (storage: demo_memory.db)\n",
            "   🏛️  Stored in long-term memory: bd3dd89f... (semantic)\n",
            "   🏛️  Stored in long-term memory: 5b1ef827... (semantic)\n",
            "   🏛️  Stored in long-term memory: b9b32520... (semantic)\n",
            "   🏛️  Stored in long-term memory: f52c8999... (episodic)\n",
            "   🏛️  Stored in long-term memory: 88a1db5c... (episodic)\n",
            "   🏛️  Stored in long-term memory: 8305e5ba... (episodic)\n",
            "   🏛️  Stored in long-term memory: 24b1e71f... (procedural)\n",
            "   🏛️  Stored in long-term memory: 92f0c601... (procedural)\n",
            "   🏛️  Stored in long-term memory: 2d97d863... (procedural)\n",
            "   ✅ Stored 3 semantic memories\n",
            "   ✅ Stored 3 episodic memories\n",
            "   ✅ Stored 3 procedural memories\n",
            "   ✅ Created memory associations\n",
            "   ✅ Search found 1 relevant memories\n",
            "   LTM total memories: 9\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Create integrated memory system\n",
        "print(\"📝 Step 3: Creating integrated memory system...\")\n",
        "\n",
        "memory_system = MemorySystem(\n",
        "    working_memory_capacity=7,\n",
        "    ltm_storage_path=\"integrated_memory.db\"\n",
        ")\n",
        "\n",
        "# Test intelligent memory routing\n",
        "test_inputs = [\n",
        "    (\"Remember my name is Alice\", MemoryType.EPISODIC, MemoryPriority.HIGH),\n",
        "    (\"The sky is blue\", MemoryType.SEMANTIC, MemoryPriority.NORMAL),\n",
        "    (\"Current task: process documents\", MemoryType.WORKING, MemoryPriority.NORMAL),\n",
        "    (\"How to make tea: boil water, add tea, steep\", MemoryType.PROCEDURAL, MemoryPriority.HIGH),\n",
        "    (\"Feeling excited about this project\", MemoryType.EMOTIONAL, MemoryPriority.NORMAL)\n",
        "]\n",
        "\n",
        "stored_memory_ids = []\n",
        "for content, mem_type, priority in test_inputs:\n",
        "    memory_id = memory_system.store(\n",
        "        content=content,\n",
        "        memory_type=mem_type,\n",
        "        priority=priority,\n",
        "        context={\"demo\": True, \"step\": 3},\n",
        "        tags={\"integrated_test\"}\n",
        "    )\n",
        "    stored_memory_ids.append(memory_id)\n",
        "\n",
        "print(f\"   ✅ Stored {len(stored_memory_ids)} memories with intelligent routing\")\n",
        "\n",
        "# Test cross-memory search\n",
        "unified_query = MemoryQuery(\n",
        "    content_query=\"task\",\n",
        "    memory_types=list(MemoryType),\n",
        "    context_filter={\"demo\": True},\n",
        "    limit=10\n",
        ")\n",
        "\n",
        "unified_results = memory_system.search(unified_query)\n",
        "print(f\"   ✅ Unified search found {len(unified_results)} memories across all systems\")\n",
        "\n",
        "# Test memory associations\n",
        "if len(stored_memory_ids) >= 2:\n",
        "    memory_system.associate(\n",
        "        stored_memory_ids[0],  # Alice\n",
        "        stored_memory_ids[4],  # Excitement\n",
        "        \"emotional_connection\",\n",
        "        strength=0.7\n",
        "    )\n",
        "    print(\"   ✅ Created cross-memory associations\")\n",
        "\n",
        "# Get initial memory statistics\n",
        "initial_stats = memory_system.get_memory_statistics()\n",
        "print(f\"   Initial system state: {initial_stats['total_memories_created']} total memories\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vny4dipj_jaL",
        "outputId": "2ac2f030-9886-4110-ae88-2d5edfa388b4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 3: Creating integrated memory system...\n",
            "🧠 Working memory initialized (capacity: 7)\n",
            "   📚 Loaded 0 memories from database\n",
            "🏛️  Long-term memory initialized (storage: integrated_memory.db)\n",
            "🧠 Integrated memory system initialized\n",
            "   🏛️  Stored in long-term memory: 8ffa4140... (episodic)\n",
            "   🏛️  Stored in long-term memory: 4cf0a24e... (semantic)\n",
            "   💭 Stored in working memory: dbdd9bcd...\n",
            "   🏛️  Stored in long-term memory: ab382412... (procedural)\n",
            "   💭 Stored in working memory: 7926f7e2...\n",
            "   ✅ Stored 5 memories with intelligent routing\n",
            "   ✅ Unified search found 1 memories across all systems\n",
            "   ✅ Created cross-memory associations\n",
            "   Initial system state: 5 total memories\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Create and test memory-aware agent\n",
        "print(\"📝 Step 4: Creating and testing memory-aware agent...\")\n",
        "\n",
        "# Create memory-aware agent\n",
        "agent = MemoryAwareAgent(\n",
        "    name=\"demo_agent\",\n",
        "    memory_system=memory_system\n",
        ")\n",
        "\n",
        "# Test memory-enhanced processing\n",
        "test_conversations = [\n",
        "    \"Hello, I'm working on a Python project\",\n",
        "    \"I need help with debugging my code\",\n",
        "    \"Can you help me make some tea?\",\n",
        "    \"I'm feeling stuck on this programming task\",\n",
        "    \"What did we talk about earlier regarding Python?\"\n",
        "]\n",
        "\n",
        "print(\"   Testing conversational memory...\")\n",
        "conversation_results = []\n",
        "for i, message in enumerate(test_conversations):\n",
        "    print(f\"     User: {message}\")\n",
        "\n",
        "    response = agent.process_with_memory(\n",
        "        input_data=message,\n",
        "        context={\"conversation_turn\": i, \"user\": \"demo_user\"}\n",
        "    )\n",
        "\n",
        "    conversation_results.append(response)\n",
        "    print(f\"     Agent: {response[:100]}{'...' if len(response) > 100 else ''}\")\n",
        "    print()\n",
        "\n",
        "# Analyze learning\n",
        "learning_summary = agent.get_learning_summary()\n",
        "print(f\"   ✅ Agent learned {learning_summary['meaningful_patterns']} meaningful patterns\")\n",
        "print(f\"   ✅ Processed {learning_summary['learning_events']} learning events\")\n",
        "\n",
        "# Test conversation recall\n",
        "print(\"   Testing conversation recall...\")\n",
        "recent_conversation = agent.recall_conversation(limit=3)\n",
        "print(f\"   ✅ Recalled {len(recent_conversation)} recent exchanges\")\n",
        "\n",
        "# Test related memory retrieval\n",
        "if stored_memory_ids:\n",
        "    related_memories = memory_system.get_related_memories(stored_memory_ids[0], max_results=3)\n",
        "    print(f\"   ✅ Found {len(related_memories)} related memories\")\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgSYA5po_q2v",
        "outputId": "65caba5a-2cf8-42fc-a9c2-0c288fc0849a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 4: Creating and testing memory-aware agent...\n",
            "🧠 Created memory-aware agent: demo_agent\n",
            "   Testing conversational memory...\n",
            "     User: Hello, I'm working on a Python project\n",
            "   💭 Stored in working memory: 08293a24...\n",
            "   💭 Stored in working memory: cc6cebef...\n",
            "     Agent: Processed: Hello, I'm working on a Python project\n",
            "\n",
            "     User: I need help with debugging my code\n",
            "   💭 Stored in working memory: 32ce9db6...\n",
            "   💭 Stored in working memory: 878b5036...\n",
            "     Agent: Processed: I need help with debugging my code\n",
            "\n",
            "     User: Can you help me make some tea?\n",
            "   💭 Stored in working memory: 73d5a60e...\n",
            "   🔄 Evicted dbdd9bcd... from working memory\n",
            "   💭 Stored in working memory: 31640285...\n",
            "     Agent: Processed: Can you help me make some tea?\n",
            "\n",
            "     User: I'm feeling stuck on this programming task\n",
            "   🔄 Evicted 7926f7e2... from working memory\n",
            "   💭 Stored in working memory: 19839652...\n",
            "   🔄 Evicted 08293a24... from working memory\n",
            "   💭 Stored in working memory: a7c5af59...\n",
            "     Agent: Processed: I'm feeling stuck on this programming task\n",
            "\n",
            "     User: What did we talk about earlier regarding Python?\n",
            "   🔄 Evicted cc6cebef... from working memory\n",
            "   💭 Stored in working memory: c32e1a50...\n",
            "   🔄 Evicted 32ce9db6... from working memory\n",
            "   💭 Stored in working memory: b0576753...\n",
            "     Agent: Processed: What did we talk about earlier regarding Python?\n",
            "\n",
            "   ✅ Agent learned 1 meaningful patterns\n",
            "   ✅ Processed 5 learning events\n",
            "   Testing conversation recall...\n",
            "   ✅ Recalled 3 recent exchanges\n",
            "   ✅ Found 0 related memories\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Create and test knowledge graph\n",
        "print(\"📝 Step 5: Creating and testing knowledge graph...\")\n",
        "\n",
        "knowledge_graph = KnowledgeGraph()\n",
        "\n",
        "# Add entities\n",
        "entities = [\n",
        "    (\"python\", \"programming_language\", {\"created\": 1991, \"type\": \"interpreted\"}),\n",
        "    (\"alice\", \"person\", {\"role\": \"developer\", \"experience\": \"senior\"}),\n",
        "    (\"debugging\", \"skill\", {\"difficulty\": \"intermediate\", \"importance\": \"high\"}),\n",
        "    (\"machine_learning\", \"field\", {\"complexity\": \"high\", \"growth\": \"rapid\"}),\n",
        "    (\"neural_networks\", \"technology\", {\"type\": \"ai\", \"applications\": \"many\"})\n",
        "]\n",
        "\n",
        "for entity_id, entity_type, attributes in entities:\n",
        "    knowledge_graph.add_entity(entity_id, entity_type, attributes)\n",
        "\n",
        "# Add relationships\n",
        "relationships = [\n",
        "    (\"alice\", \"knows\", \"python\", {\"proficiency\": \"expert\"}),\n",
        "    (\"alice\", \"uses\", \"debugging\", {\"frequency\": \"daily\"}),\n",
        "    (\"python\", \"supports\", \"machine_learning\", {\"via\": \"libraries\"}),\n",
        "    (\"machine_learning\", \"uses\", \"neural_networks\", {\"commonly\": True}),\n",
        "    (\"debugging\", \"required_for\", \"python\", {\"always\": True})\n",
        "]\n",
        "\n",
        "for source, rel_type, target, properties in relationships:\n",
        "    knowledge_graph.add_relationship(source, rel_type, target, properties)\n",
        "\n",
        "print(f\"   ✅ Created knowledge graph with {len(entities)} entities and {len(relationships)} relationships\")\n",
        "\n",
        "# Test knowledge graph queries\n",
        "python_related = knowledge_graph.get_neighbors(\"python\")\n",
        "print(f\"   ✅ Found {len(python_related)} entities related to Python: {python_related}\")\n",
        "\n",
        "# Test path finding\n",
        "path = knowledge_graph.get_path(\"alice\", \"neural_networks\")\n",
        "if path:\n",
        "    print(f\"   ✅ Found path from Alice to Neural Networks: {' -> '.join(path)}\")\n",
        "\n",
        "kg_stats = knowledge_graph.get_statistics()\n",
        "print(f\"   Knowledge graph stats: {kg_stats['total_entities']} entities, {kg_stats['total_relationships']} relationships\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLdaJYfF_vq3",
        "outputId": "b0e0f05c-130a-465f-c19c-2a83fca72c0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 5: Creating and testing knowledge graph...\n",
            "🕸️  Knowledge graph initialized\n",
            "   🏷️  Added entity: python (programming_language)\n",
            "   🏷️  Added entity: alice (person)\n",
            "   🏷️  Added entity: debugging (skill)\n",
            "   🏷️  Added entity: machine_learning (field)\n",
            "   🏷️  Added entity: neural_networks (technology)\n",
            "   🔗 Added relationship: alice --knows--> python\n",
            "   🔗 Added relationship: alice --uses--> debugging\n",
            "   🔗 Added relationship: python --supports--> machine_learning\n",
            "   🔗 Added relationship: machine_learning --uses--> neural_networks\n",
            "   🔗 Added relationship: debugging --required_for--> python\n",
            "   ✅ Created knowledge graph with 5 entities and 5 relationships\n",
            "   ✅ Found 3 entities related to Python: ['alice', 'machine_learning', 'debugging']\n",
            "   Knowledge graph stats: 5 entities, 5 relationships\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Memory system performance testing\n",
        "print(\"📝 Step 6: Testing memory system performance...\")\n",
        "\n",
        "# Generate load for performance testing\n",
        "import random\n",
        "\n",
        "print(\"   Generating memory load for performance testing...\")\n",
        "performance_test_ids = []\n",
        "\n",
        "# Create diverse memory content\n",
        "content_templates = [\n",
        "    \"User {user} performed action {action} at time {time}\",\n",
        "    \"System learned pattern: {pattern} with confidence {confidence}\",\n",
        "    \"Knowledge fact: {subject} has property {property} with value {value}\",\n",
        "    \"Procedure step {step}: {instruction}\",\n",
        "    \"Emotional event: feeling {emotion} about {subject}\"\n",
        "]\n",
        "\n",
        "users = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\"]\n",
        "actions = [\"login\", \"search\", \"create\", \"update\", \"delete\"]\n",
        "patterns = [\"sequence_A\", \"behavior_B\", \"trend_C\", \"anomaly_D\"]\n",
        "subjects = [\"system\", \"user\", \"data\", \"process\", \"interface\"]\n",
        "emotions = [\"happy\", \"frustrated\", \"excited\", \"confused\", \"satisfied\"]\n",
        "\n",
        "# Generate 50 diverse memories\n",
        "for i in range(50):\n",
        "    template = random.choice(content_templates)\n",
        "\n",
        "    if \"User\" in template:\n",
        "        content = template.format(\n",
        "            user=random.choice(users),\n",
        "            action=random.choice(actions),\n",
        "            time=f\"2025-05-24T{random.randint(9, 17):02d}:{random.randint(0, 59):02d}\"\n",
        "        )\n",
        "        mem_type = MemoryType.EPISODIC\n",
        "    elif \"learned pattern\" in template:\n",
        "        content = template.format(\n",
        "            pattern=random.choice(patterns),\n",
        "            confidence=f\"{random.uniform(0.6, 0.9):.2f}\"\n",
        "        )\n",
        "        mem_type = MemoryType.SEMANTIC\n",
        "    elif \"Knowledge fact\" in template:\n",
        "        content = template.format(\n",
        "            subject=random.choice(subjects),\n",
        "            property=random.choice([\"color\", \"size\", \"speed\", \"type\"]),\n",
        "            value=random.choice([\"blue\", \"large\", \"fast\", \"complex\"])\n",
        "        )\n",
        "        mem_type = MemoryType.SEMANTIC\n",
        "    elif \"Procedure step\" in template:\n",
        "        content = template.format(\n",
        "            step=random.randint(1, 5),\n",
        "            instruction=f\"Execute {random.choice(actions)} operation\"\n",
        "        )\n",
        "        mem_type = MemoryType.PROCEDURAL\n",
        "    else:  # Emotional\n",
        "        content = template.format(\n",
        "            emotion=random.choice(emotions),\n",
        "            subject=random.choice(subjects)\n",
        "        )\n",
        "        mem_type = MemoryType.EMOTIONAL\n",
        "\n",
        "    memory_id = memory_system.store(\n",
        "        content=content,\n",
        "        memory_type=mem_type,\n",
        "        priority=random.choice(list(MemoryPriority)),\n",
        "        context={\n",
        "            \"test_batch\": \"performance\",\n",
        "            \"index\": i,\n",
        "            \"category\": random.choice([\"A\", \"B\", \"C\"])\n",
        "        },\n",
        "        tags={f\"tag_{random.randint(1, 5)}\", \"performance_test\"},\n",
        "        emotional_valence=random.uniform(-0.5, 0.5),\n",
        "        importance_score=random.uniform(0.2, 0.9)\n",
        "    )\n",
        "    performance_test_ids.append(memory_id)\n",
        "\n",
        "print(f\"   ✅ Generated {len(performance_test_ids)} test memories\")\n",
        "\n",
        "# Wait for consolidation to process\n",
        "time.sleep(2)\n",
        "\n",
        "# Test search performance\n",
        "print(\"   Testing search performance...\")\n",
        "search_start_time = time.time()\n",
        "\n",
        "test_queries = [\n",
        "    MemoryQuery(content_query=\"Alice\", limit=10),\n",
        "    MemoryQuery(memory_types=[MemoryType.EPISODIC], limit=20),\n",
        "    MemoryQuery(context_filter={\"category\": \"A\"}, limit=15),\n",
        "    MemoryQuery(content_query=\"pattern\", memory_types=[MemoryType.SEMANTIC], limit=5)\n",
        "]\n",
        "\n",
        "total_results = 0\n",
        "for query in test_queries:\n",
        "    results = memory_system.search(query)\n",
        "    total_results += len(results)\n",
        "\n",
        "search_time = time.time() - search_start_time\n",
        "print(f\"   ✅ Search performance: {total_results} results in {search_time:.3f}s\")\n",
        "\n",
        "# Test retrieval performance\n",
        "print(\"   Testing retrieval performance...\")\n",
        "retrieval_start_time = time.time()\n",
        "\n",
        "# Randomly retrieve 20 memories\n",
        "sample_ids = random.sample(performance_test_ids, min(20, len(performance_test_ids)))\n",
        "retrieved_count = 0\n",
        "for memory_id in sample_ids:\n",
        "    memory = memory_system.retrieve(memory_id)\n",
        "    if memory:\n",
        "        retrieved_count += 1\n",
        "\n",
        "retrieval_time = time.time() - retrieval_start_time\n",
        "print(f\"   ✅ Retrieval performance: {retrieved_count} memories in {retrieval_time:.3f}s\")\n",
        "\n",
        "# Memory system statistics after load\n",
        "final_stats = memory_system.get_memory_statistics()\n",
        "print(f\"   Final system state: {final_stats['total_memories_created']} total memories\")\n",
        "print(f\"   Consolidations: {final_stats['consolidation_count']}\")\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMZGvf5g_0dX",
        "outputId": "6455b18e-7745-4a69-9140-e6d8d0f62fb3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 6: Testing memory system performance...\n",
            "   Generating memory load for performance testing...\n",
            "   🏛️  Stored in long-term memory: 18ad7b96... (procedural)\n",
            "   🏛️  Stored in long-term memory: 11b0a21b... (semantic)\n",
            "   🏛️  Stored in long-term memory: 5c0bf421... (semantic)\n",
            "   🏛️  Stored in long-term memory: a878e741... (semantic)\n",
            "   🏛️  Stored in long-term memory: 1017669a... (semantic)\n",
            "   🏛️  Stored in long-term memory: 9517955d... (semantic)\n",
            "   🏛️  Stored in long-term memory: bd31b225... (semantic)\n",
            "   🏛️  Stored in long-term memory: a9347216... (semantic)\n",
            "   🏛️  Stored in long-term memory: fdf667fa... (semantic)\n",
            "   🏛️  Stored in long-term memory: e85c0367... (episodic)\n",
            "   🏛️  Stored in long-term memory: b064ebce... (semantic)\n",
            "   🏛️  Stored in long-term memory: 1d328ba5... (emotional)\n",
            "   🏛️  Stored in long-term memory: ad269d71... (semantic)\n",
            "   🏛️  Stored in long-term memory: 5edc73df... (emotional)\n",
            "   🏛️  Stored in long-term memory: bebbfec5... (semantic)\n",
            "   🔄 Evicted 878b5036... from working memory\n",
            "   💭 Stored in working memory: 89d5fc29...\n",
            "   🔄 Evicted 73d5a60e... from working memory\n",
            "   💭 Stored in working memory: ba71fbc0...\n",
            "   🏛️  Stored in long-term memory: 6e1450b7... (emotional)\n",
            "   🏛️  Stored in long-term memory: 09dbed0f... (procedural)\n",
            "   🔄 Evicted 31640285... from working memory\n",
            "   💭 Stored in working memory: 63839582...\n",
            "   🔄 Evicted 19839652... from working memory\n",
            "   💭 Stored in working memory: e5bcccee...\n",
            "   🏛️  Stored in long-term memory: 8dde6e24... (procedural)\n",
            "   🔄 Evicted a7c5af59... from working memory\n",
            "   💭 Stored in working memory: fe509dd9...\n",
            "   🏛️  Stored in long-term memory: a252a8b9... (semantic)\n",
            "   🏛️  Stored in long-term memory: f04f2dd8... (semantic)\n",
            "   🔄 Evicted c32e1a50... from working memory\n",
            "   💭 Stored in working memory: 9f3acf25...\n",
            "   🏛️  Stored in long-term memory: c9fea84b... (emotional)\n",
            "   🏛️  Stored in long-term memory: 28dc571a... (emotional)\n",
            "   🔄 Evicted b0576753... from working memory\n",
            "   💭 Stored in working memory: 58e9a7db...\n",
            "   🔄 Evicted 89d5fc29... from working memory\n",
            "   💭 Stored in working memory: 9f83b79a...\n",
            "   🏛️  Stored in long-term memory: f828cb4d... (semantic)\n",
            "   🏛️  Stored in long-term memory: d7b6092e... (episodic)\n",
            "   🏛️  Stored in long-term memory: 4761e9ff... (semantic)\n",
            "   🏛️  Stored in long-term memory: 1898d1c2... (semantic)\n",
            "   🏛️  Stored in long-term memory: 4f10b99a... (procedural)\n",
            "   🔄 Evicted ba71fbc0... from working memory\n",
            "   💭 Stored in working memory: e69374c3...\n",
            "   🏛️  Stored in long-term memory: 25c55f10... (procedural)\n",
            "   🏛️  Stored in long-term memory: a6fe8b7b... (emotional)\n",
            "   🏛️  Stored in long-term memory: e0deb660... (emotional)\n",
            "   🔄 Evicted 63839582... from working memory\n",
            "   💭 Stored in working memory: 1875cc08...\n",
            "   🏛️  Stored in long-term memory: 173c8e74... (semantic)\n",
            "   🏛️  Stored in long-term memory: aa8b763a... (semantic)\n",
            "   🏛️  Stored in long-term memory: 0658ae75... (semantic)\n",
            "   🔄 Evicted e5bcccee... from working memory\n",
            "   💭 Stored in working memory: a3206938...\n",
            "   🔄 Evicted fe509dd9... from working memory\n",
            "   💭 Stored in working memory: ecf21c25...\n",
            "   🏛️  Stored in long-term memory: eff8bac8... (semantic)\n",
            "   🏛️  Stored in long-term memory: ac88e718... (procedural)\n",
            "   🏛️  Stored in long-term memory: 15522dde... (semantic)\n",
            "   🏛️  Stored in long-term memory: c8d1a505... (semantic)\n",
            "   🏛️  Stored in long-term memory: b34fcfc7... (semantic)\n",
            "   ✅ Generated 50 test memories\n",
            "   Testing search performance...\n",
            "   ✅ Search performance: 25 results in 0.001s\n",
            "   Testing retrieval performance...\n",
            "   ✅ Retrieval performance: 17 memories in 0.122s\n",
            "   Final system state: 65 total memories\n",
            "   Consolidations: 0\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Memory visualization and analysis\n",
        "print(\"📝 Step 7: Creating memory visualization and analysis...\")\n",
        "\n",
        "visualizer = MemoryVisualizer(memory_system)\n",
        "\n",
        "# Generate comprehensive dashboard\n",
        "dashboard = visualizer.create_memory_dashboard()\n",
        "print(\"   ✅ Generated memory dashboard\")\n",
        "\n",
        "# Analyze memory patterns\n",
        "patterns = visualizer.analyze_memory_patterns(limit=50)\n",
        "print(f\"   ✅ Analyzed {patterns.get('sample_size', 0)} memories for patterns\")\n",
        "\n",
        "# Generate human-readable report\n",
        "report = visualizer.generate_memory_report()\n",
        "print(\"   ✅ Generated comprehensive memory report\")\n",
        "\n",
        "# Display key insights from dashboard\n",
        "overview = dashboard['overview']\n",
        "print(f\"   Key insights:\")\n",
        "print(f\"     Consolidation rate: {overview['consolidation_rate']:.1%}\")\n",
        "print(f\"     System efficiency: {dashboard['working_memory_efficiency']['utilization']:.1f}%\")\n",
        "\n",
        "# Display memory distribution\n",
        "distribution = dashboard['memory_distribution']['by_type_percentages']\n",
        "print(f\"     Memory type distribution:\")\n",
        "for mem_type, percentage in distribution.items():\n",
        "    print(f\"       {mem_type.capitalize()}: {percentage:.1f}%\")\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1CH_eNj__7nQ",
        "outputId": "d6c3b6b0-2e86-46b2-8571-06ab66705b15"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 7: Creating memory visualization and analysis...\n",
            "📊 Memory visualizer initialized\n",
            "   ✅ Generated memory dashboard\n",
            "   ✅ Analyzed 48 memories for patterns\n",
            "   ✅ Generated comprehensive memory report\n",
            "   Key insights:\n",
            "     Consolidation rate: 0.0%\n",
            "     System efficiency: 100.0%\n",
            "     Memory type distribution:\n",
            "       Working: 0.0%\n",
            "       Episodic: 7.3%\n",
            "       Semantic: 58.5%\n",
            "       Procedural: 17.1%\n",
            "       Emotional: 17.1%\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Advanced memory features demonstration\n",
        "print(\"📝 Step 8: Demonstrating advanced memory features...\")\n",
        "\n",
        "# Test memory decay simulation\n",
        "print(\"   Testing memory decay simulation...\")\n",
        "weak_memory_id = memory_system.store(\n",
        "    content=\"This memory should decay quickly\",\n",
        "    memory_type=MemoryType.WORKING,\n",
        "    priority=MemoryPriority.TRANSIENT,\n",
        "    importance_score=0.1\n",
        ")\n",
        "\n",
        "# Time passage and access pattern\n",
        "original_memory = memory_system.retrieve(weak_memory_id)\n",
        "if original_memory:\n",
        "    original_strength = original_memory.strength\n",
        "\n",
        "    # Simulate decay\n",
        "    original_memory.decay(decay_rate=0.1)\n",
        "    print(f\"     Memory strength: {original_strength:.2f} → {original_memory.strength:.2f}\")\n",
        "\n",
        "# Test emotional memory associations\n",
        "print(\"   Testing emotional memory associations...\")\n",
        "emotional_memories = []\n",
        "\n",
        "emotional_content = [\n",
        "    (\"Won the programming contest!\", 0.8, 0.9),\n",
        "    (\"Had a difficult debugging session\", -0.3, 0.7),\n",
        "    (\"Learned something new today\", 0.5, 0.8),\n",
        "    (\"Code review went well\", 0.4, 0.6)\n",
        "]\n",
        "\n",
        "for content, valence, importance in emotional_content:\n",
        "    memory_id = memory_system.store(\n",
        "        content=content,\n",
        "        memory_type=MemoryType.EMOTIONAL,\n",
        "        emotional_valence=valence,\n",
        "        importance_score=importance,\n",
        "        tags={\"emotional\", \"experience\"}\n",
        "    )\n",
        "    emotional_memories.append(memory_id)\n",
        "\n",
        "# Create emotional associations\n",
        "for i in range(len(emotional_memories) - 1):\n",
        "    memory_system.associate(\n",
        "        emotional_memories[i],\n",
        "        emotional_memories[i + 1],\n",
        "        \"temporal_sequence\",\n",
        "        strength=0.6\n",
        "    )\n",
        "\n",
        "print(f\"   ✅ Created {len(emotional_memories)} emotional memories with associations\")\n",
        "\n",
        "# Test memory consolidation monitoring\n",
        "print(\"   Testing memory consolidation monitoring...\")\n",
        "consolidation_stats = memory_system.get_memory_statistics()\n",
        "print(f\"     Active consolidation queue: {consolidation_stats['consolidation_queue_size']} items\")\n",
        "print(f\"     Total consolidations: {consolidation_stats['consolidation_count']}\")\n",
        "\n",
        "# Test agent learning and pattern recognition\n",
        "print(\"   Testing advanced agent learning...\")\n",
        "learning_agent = MemoryAwareAgent(name=\"learning_agent\", memory_system=memory_system)\n",
        "\n",
        "# Feed structured learning data\n",
        "learning_scenarios = [\n",
        "    (\"When I see error X, I should check Y\", {\"scenario\": \"debugging\", \"action\": \"check\"}),\n",
        "    (\"User Alice prefers feature A\", {\"user\": \"Alice\", \"preference\": \"feature_A\"}),\n",
        "    (\"Pattern B usually indicates problem C\", {\"pattern\": \"B\", \"indicates\": \"problem_C\"}),\n",
        "    (\"When debugging error X, checking Y works\", {\"scenario\": \"debugging\", \"success\": True}),\n",
        "    (\"Alice requested feature A again\", {\"user\": \"Alice\", \"repeated\": True})\n",
        "]\n",
        "\n",
        "for scenario, context in learning_scenarios:\n",
        "    learning_agent.process_with_memory(scenario, context)\n",
        "\n",
        "# Check what the agent learned\n",
        "agent_learning = learning_agent.get_learning_summary()\n",
        "print(f\"     Agent learned {agent_learning['meaningful_patterns']} patterns\")\n",
        "print(f\"     Processing events: {agent_learning['learning_events']}\")\n",
        "\n",
        "if agent_learning['pattern_details']:\n",
        "    print(\"     Top learned patterns:\")\n",
        "    for pattern, details in list(agent_learning['pattern_details'].items())[:3]:\n",
        "        print(f\"       {pattern}: {details['frequency']} occurrences\")\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg2V_SJqAARW",
        "outputId": "bdfb7772-1a1b-4990-94bb-76996a729dee"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 8: Demonstrating advanced memory features...\n",
            "   Testing memory decay simulation...\n",
            "   🔄 Evicted ecf21c25... from working memory\n",
            "   💭 Stored in working memory: 922dd39f...\n",
            "     Memory strength: 1.00 → 1.00\n",
            "   Testing emotional memory associations...\n",
            "   🏛️  Stored in long-term memory: 137a055d... (emotional)\n",
            "   🔄 Evicted 58e9a7db... from working memory\n",
            "   💭 Stored in working memory: 50258a5e...\n",
            "   🏛️  Stored in long-term memory: ae4f5f18... (emotional)\n",
            "   🔄 Evicted e69374c3... from working memory\n",
            "   💭 Stored in working memory: 6a8e2bf1...\n",
            "   ✅ Created 4 emotional memories with associations\n",
            "   Testing memory consolidation monitoring...\n",
            "     Active consolidation queue: 2 items\n",
            "     Total consolidations: 0\n",
            "   Testing advanced agent learning...\n",
            "🧠 Created memory-aware agent: learning_agent\n",
            "   🔄 Evicted 1875cc08... from working memory\n",
            "   💭 Stored in working memory: 1213e95b...\n",
            "   🔄 Evicted a3206938... from working memory\n",
            "   💭 Stored in working memory: 7f8cf879...\n",
            "   🔄 Evicted 9f3acf25... from working memory\n",
            "   💭 Stored in working memory: bab32cbe...\n",
            "   🔄 Evicted 9f83b79a... from working memory\n",
            "   💭 Stored in working memory: 4c0973f6...\n",
            "   🔄 Evicted 922dd39f... from working memory\n",
            "   💭 Stored in working memory: 03a24b2a...\n",
            "   🔄 Evicted 50258a5e... from working memory\n",
            "   💭 Stored in working memory: c3d78764...\n",
            "   🔄 Evicted 6a8e2bf1... from working memory\n",
            "   💭 Stored in working memory: 26122bff...\n",
            "   🔄 Evicted 1213e95b... from working memory\n",
            "   💭 Stored in working memory: b90a2cac...\n",
            "   🔄 Evicted 7f8cf879... from working memory\n",
            "   💭 Stored in working memory: 1b289ae5...\n",
            "   🔄 Evicted bab32cbe... from working memory\n",
            "   💭 Stored in working memory: 87034bea...\n",
            "     Agent learned 4 patterns\n",
            "     Processing events: 5\n",
            "     Top learned patterns:\n",
            "       input_pattern_when: 2 occurrences\n",
            "       input_pattern_error: 2 occurrences\n",
            "       input_pattern_alice: 2 occurrences\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Memory system integration testing\n",
        "print(\"📝 Step 9: Testing memory system integration...\")\n",
        "\n",
        "# Test cross-system memory sharing\n",
        "print(\"   Testing cross-system memory sharing...\")\n",
        "\n",
        "# Create multiple agents sharing the same memory system\n",
        "agents = []\n",
        "for i in range(3):\n",
        "    agent = MemoryAwareAgent(\n",
        "        name=f\"shared_agent_{i}\",\n",
        "        memory_system=memory_system\n",
        "    )\n",
        "    agents.append(agent)\n",
        "\n",
        "# Have agents interact and build shared memories\n",
        "shared_interactions = [\n",
        "    (\"Agent 0 says: I found a bug in module A\", {\"speaker\": \"agent_0\", \"topic\": \"bug\"}),\n",
        "    (\"Agent 1 says: I can help fix module A\", {\"speaker\": \"agent_1\", \"topic\": \"fix\"}),\n",
        "    (\"Agent 2 says: Module A is now working\", {\"speaker\": \"agent_2\", \"topic\": \"success\"})\n",
        "]\n",
        "\n",
        "for i, (interaction, context) in enumerate(shared_interactions):\n",
        "    agent = agents[i % len(agents)]\n",
        "    agent.process_with_memory(interaction, context)\n",
        "\n",
        "print(f\"   ✅ {len(agents)} agents shared {len(shared_interactions)} interactions\")\n",
        "\n",
        "# Test memory search across shared context\n",
        "shared_query = MemoryQuery(\n",
        "    content_query=\"module A\",\n",
        "    context_filter={\"topic\": \"bug\"},\n",
        "    limit=5\n",
        ")\n",
        "\n",
        "shared_results = memory_system.search(shared_query)\n",
        "print(f\"   ✅ Found {len(shared_results)} shared memories about module A\")\n",
        "\n",
        "# Test system resilience\n",
        "print(\"   Testing system resilience...\")\n",
        "\n",
        "# Simulate high load\n",
        "rapid_storage_count = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for i in range(20):\n",
        "    memory_id = memory_system.store(\n",
        "        content=f\"Rapid test memory {i}\",\n",
        "        memory_type=random.choice(list(MemoryType)),\n",
        "        priority=random.choice(list(MemoryPriority))\n",
        "    )\n",
        "    if memory_id:\n",
        "        rapid_storage_count += 1\n",
        "\n",
        "load_time = time.time() - start_time\n",
        "print(f\"   ✅ Stored {rapid_storage_count} memories under load in {load_time:.3f}s\")\n",
        "\n",
        "# Test error handling\n",
        "print(\"   Testing error handling...\")\n",
        "try:\n",
        "    # Try to retrieve non-existent memory\n",
        "    fake_memory = memory_system.retrieve(\"fake_memory_id\")\n",
        "    print(f\"     Error handling: {'✅ Graceful' if fake_memory is None else '❌ Failed'}\")\n",
        "\n",
        "    # Try to search with invalid query\n",
        "    empty_results = memory_system.search(MemoryQuery(content_query=\"\"))\n",
        "    print(f\"     Empty query handling: ✅ Returned {len(empty_results)} results\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"     ❌ Error handling failed: {e}\")\n",
        "\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fHYT262AM3y",
        "outputId": "fc8528de-c0c6-4c2d-d904-e8655d1e2844"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 9: Testing memory system integration...\n",
            "   Testing cross-system memory sharing...\n",
            "🧠 Created memory-aware agent: shared_agent_0\n",
            "🧠 Created memory-aware agent: shared_agent_1\n",
            "🧠 Created memory-aware agent: shared_agent_2\n",
            "   🔄 Evicted 4c0973f6... from working memory\n",
            "   💭 Stored in working memory: 77bbf167...\n",
            "   🔄 Evicted 03a24b2a... from working memory\n",
            "   💭 Stored in working memory: b6e6e4eb...\n",
            "   🔄 Evicted c3d78764... from working memory\n",
            "   💭 Stored in working memory: edb0971c...\n",
            "   🔄 Evicted 26122bff... from working memory\n",
            "   💭 Stored in working memory: affe3365...\n",
            "   🔄 Evicted b90a2cac... from working memory\n",
            "   💭 Stored in working memory: 561622ab...\n",
            "   🔄 Evicted 1b289ae5... from working memory\n",
            "   💭 Stored in working memory: bc73428d...\n",
            "   ✅ 3 agents shared 3 interactions\n",
            "   ✅ Found 5 shared memories about module A\n",
            "   Testing system resilience...\n",
            "   🏛️  Stored in long-term memory: eac2afd3... (semantic)\n",
            "   🏛️  Stored in long-term memory: 9b68f6cf... (episodic)\n",
            "   🏛️  Stored in long-term memory: 9e968ad5... (procedural)\n",
            "   🔄 Evicted 87034bea... from working memory\n",
            "   💭 Stored in working memory: aab7e2e9...\n",
            "   🔄 Evicted 77bbf167... from working memory\n",
            "   💭 Stored in working memory: 78dfc726...\n",
            "   🔄 Evicted b6e6e4eb... from working memory\n",
            "   💭 Stored in working memory: 603e7dc6...\n",
            "   🔄 Evicted edb0971c... from working memory\n",
            "   💭 Stored in working memory: 26496c86...\n",
            "   🏛️  Stored in long-term memory: caf44837... (semantic)\n",
            "   🔄 Evicted affe3365... from working memory\n",
            "   💭 Stored in working memory: 05fba9ea...\n",
            "   🔄 Evicted 561622ab... from working memory\n",
            "   💭 Stored in working memory: f3d2783e...\n",
            "   🏛️  Stored in long-term memory: 178e71e3... (procedural)\n",
            "   🏛️  Stored in long-term memory: 9535d143... (procedural)\n",
            "   🏛️  Stored in long-term memory: 8faa0cf6... (procedural)\n",
            "   🏛️  Stored in long-term memory: fd8c3039... (episodic)\n",
            "   🏛️  Stored in long-term memory: 17f25a93... (procedural)\n",
            "   🏛️  Stored in long-term memory: 044dfaed... (procedural)\n",
            "   🔄 Evicted bc73428d... from working memory\n",
            "   💭 Stored in working memory: a305b8f7...\n",
            "   🏛️  Stored in long-term memory: bb561d73... (procedural)\n",
            "   🏛️  Stored in long-term memory: 253754bb... (semantic)\n",
            "   🏛️  Stored in long-term memory: e7f938f4... (procedural)\n",
            "   ✅ Stored 20 memories under load in 0.118s\n",
            "   Testing error handling...\n",
            "     Error handling: ✅ Graceful\n",
            "     Empty query handling: ✅ Returned 10 results\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Final analysis and cleanup\n",
        "print(\"📝 Step 10: Final analysis and cleanup...\")\n",
        "\n",
        "# Generate final comprehensive report\n",
        "print(\"   Generating final memory system report...\")\n",
        "final_report = visualizer.generate_memory_report()\n",
        "\n",
        "# Display report summary\n",
        "print(\"📋 MEMORY SYSTEM FINAL REPORT\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "# Extract key metrics from report\n",
        "lines = final_report.split('\\n')\n",
        "for line in lines:\n",
        "    if any(keyword in line.lower() for keyword in\n",
        "           ['total memories', 'consolidation rate', 'utilization', 'most accessed']):\n",
        "        print(f\"   {line.strip()}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Performance summary\n",
        "final_stats = memory_system.get_memory_statistics()\n",
        "print(\"📊 PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"   Total memories created: {final_stats['total_memories_created']}\")\n",
        "print(f\"   Successful consolidations: {final_stats['consolidation_count']}\")\n",
        "print(f\"   Total retrievals: {final_stats['retrieval_count']}\")\n",
        "print(f\"   Working memory efficiency: {final_stats['working_memory']['utilization']:.1%}\")\n",
        "print(f\"   Long-term memory size: {final_stats['long_term_memory']['total_memories']}\")\n",
        "print()\n",
        "\n",
        "# Agent learning summary\n",
        "print(\"🧠 AGENT LEARNING SUMMARY\")\n",
        "print(\"=\" * 30)\n",
        "for i, agent in enumerate(agents):\n",
        "    learning = agent.get_learning_summary()\n",
        "    print(f\"   Agent {i}: {learning['meaningful_patterns']} patterns, {learning['learning_events']} events\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Knowledge graph summary\n",
        "kg_final_stats = knowledge_graph.get_statistics()\n",
        "print(\"🕸️  KNOWLEDGE GRAPH SUMMARY\")\n",
        "print(\"=\" * 30)\n",
        "print(f\"   Entities: {kg_final_stats['total_entities']}\")\n",
        "print(f\"   Relationships: {kg_final_stats['total_relationships']}\")\n",
        "print(f\"   Entity types: {kg_final_stats['entity_types']}\")\n",
        "print()\n",
        "\n",
        "# Cleanup\n",
        "print(\"🧹 Cleaning up...\")\n",
        "memory_system.shutdown()\n",
        "ltm.shutdown()\n",
        "\n",
        "# Clean up database files\n",
        "import os\n",
        "for db_file in [\"demo_memory.db\", \"integrated_memory.db\"]:\n",
        "    if os.path.exists(db_file):\n",
        "        try:\n",
        "            os.remove(db_file)\n",
        "            print(f\"   ✅ Cleaned up {db_file}\")\n",
        "        except:\n",
        "            print(f\"   ⚠️  Could not remove {db_file}\")\n",
        "\n",
        "print(\"   ✅ Memory system cleanup complete\")\n",
        "print()\n",
        "\n",
        "print(\"✅ Tutorial 10 Complete!\")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmO9wFNBAU8w",
        "outputId": "855698b3-9960-4564-e56c-a4c9407e96c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📝 Step 10: Final analysis and cleanup...\n",
            "   Generating final memory system report...\n",
            "📋 MEMORY SYSTEM FINAL REPORT\n",
            "========================================\n",
            "   Total memories created: 106\n",
            "   Consolidation rate: 0.0%\n",
            "   Utilization: 100.0%\n",
            "   Most accessed memory: 6 accesses\n",
            "\n",
            "📊 PERFORMANCE SUMMARY\n",
            "==============================\n",
            "   Total memories created: 106\n",
            "   Successful consolidations: 0\n",
            "   Total retrievals: 25\n",
            "   Working memory efficiency: 100.0%\n",
            "   Long-term memory size: 56\n",
            "\n",
            "🧠 AGENT LEARNING SUMMARY\n",
            "==============================\n",
            "   Agent 0: 0 patterns, 1 events\n",
            "   Agent 1: 0 patterns, 1 events\n",
            "   Agent 2: 0 patterns, 1 events\n",
            "\n",
            "🕸️  KNOWLEDGE GRAPH SUMMARY\n",
            "==============================\n",
            "   Entities: 5\n",
            "   Relationships: 5\n",
            "   Entity types: 5\n",
            "\n",
            "🧹 Cleaning up...\n",
            "🛑 Memory system shutdown complete\n",
            "   ✅ Cleaned up demo_memory.db\n",
            "   ✅ Cleaned up integrated_memory.db\n",
            "   ✅ Memory system cleanup complete\n",
            "\n",
            "✅ Tutorial 10 Complete!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY OF WHAT WE LEARNED\n",
        "# =============================================================================\n",
        "\n",
        "print(\"📚 WHAT WE LEARNED:\")\n",
        "\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"1. 🧠 Built multi-layered memory architecture\")\n",
        "\n",
        "print(\"   - Working memory for active processing\")\n",
        "\n",
        "print(\"   - Long-term memory with different types (semantic, episodic, procedural, emotional)\")\n",
        "\n",
        "print(\"   - Memory consolidation and decay mechanisms\")\n",
        "\n",
        "print(\"   - Cross-memory querying and association systems\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"2. 🏗️  Implemented intelligent memory management\")\n",
        "\n",
        "print(\"   - Automatic routing between memory systems\")\n",
        "\n",
        "print(\"   - Background consolidation and maintenance\")\n",
        "\n",
        "print(\"   - Memory strength and importance modeling\")\n",
        "\n",
        "print(\"   - Persistent storage with SQLite\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"3. 🤖 Created memory-aware intelligent agents\")\n",
        "\n",
        "print(\"   - Context-driven memory retrieval\")\n",
        "\n",
        "print(\"   - Learning from experience and pattern recognition\")\n",
        "\n",
        "print(\"   - Conversation history and relationship tracking\")\n",
        "\n",
        "print(\"   - Shared memory systems for agent collaboration\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"4. 🕸️  Added knowledge graph capabilities\")\n",
        "\n",
        "print(\"   - Entity and relationship modeling\")\n",
        "\n",
        "print(\"   - Semantic memory organization\")\n",
        "\n",
        "print(\"   - Path finding and neighbor queries\")\n",
        "\n",
        "print(\"   - Structured knowledge representation\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"5. 📊 Built comprehensive analysis tools\")\n",
        "\n",
        "print(\"   - Memory system visualization and dashboards\")\n",
        "\n",
        "print(\"   - Performance monitoring and optimization\")\n",
        "\n",
        "print(\"   - Pattern analysis and reporting\")\n",
        "\n",
        "print(\"   - System health and efficiency metrics\")\n",
        "\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J50mKMSYAclK",
        "outputId": "5c8d54a9-1c88-40d5-8631-2085d2ebe46e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📚 WHAT WE LEARNED:\n",
            "========================================\n",
            "1. 🧠 Built multi-layered memory architecture\n",
            "   - Working memory for active processing\n",
            "   - Long-term memory with different types (semantic, episodic, procedural, emotional)\n",
            "   - Memory consolidation and decay mechanisms\n",
            "   - Cross-memory querying and association systems\n",
            "\n",
            "2. 🏗️  Implemented intelligent memory management\n",
            "   - Automatic routing between memory systems\n",
            "   - Background consolidation and maintenance\n",
            "   - Memory strength and importance modeling\n",
            "   - Persistent storage with SQLite\n",
            "\n",
            "3. 🤖 Created memory-aware intelligent agents\n",
            "   - Context-driven memory retrieval\n",
            "   - Learning from experience and pattern recognition\n",
            "   - Conversation history and relationship tracking\n",
            "   - Shared memory systems for agent collaboration\n",
            "\n",
            "4. 🕸️  Added knowledge graph capabilities\n",
            "   - Entity and relationship modeling\n",
            "   - Semantic memory organization\n",
            "   - Path finding and neighbor queries\n",
            "   - Structured knowledge representation\n",
            "\n",
            "5. 📊 Built comprehensive analysis tools\n",
            "   - Memory system visualization and dashboards\n",
            "   - Performance monitoring and optimization\n",
            "   - Pattern analysis and reporting\n",
            "   - System health and efficiency metrics\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# COMMON ERRORS AND SOLUTIONS\n",
        "# =============================================================================\n",
        "\n",
        "print(\"⚠️  COMMON ERRORS AND SOLUTIONS:\")\n",
        "\n",
        "print(\"=\" * 40)\n",
        "\n",
        "print(\"1. 🐛 Memory leaks and unbounded growth\")\n",
        "\n",
        "print(\"   Problem: Memory systems growing without bounds\")\n",
        "\n",
        "print(\"   Solution: Implement proper decay mechanisms and cleanup\")\n",
        "\n",
        "print(\"   Solution: Set limits on memory system sizes\")\n",
        "\n",
        "print(\"   Solution: Use background maintenance threads\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"2. 🐛 Slow memory retrieval performance\")\n",
        "\n",
        "print(\"   Problem: Linear search through large memory stores\")\n",
        "\n",
        "print(\"   Solution: Build proper indices (type, tag, context)\")\n",
        "\n",
        "print(\"   Solution: Use database storage with optimized queries\")\n",
        "\n",
        "print(\"   Solution: Cache frequently accessed memories\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"3. 🐛 Memory consolidation conflicts\")\n",
        "\n",
        "print(\"   Problem: Race conditions during consolidation\")\n",
        "\n",
        "print(\"   Solution: Use proper threading and synchronization\")\n",
        "\n",
        "print(\"   Solution: Implement consolidation queues and batching\")\n",
        "\n",
        "print(\"   Solution: Add transaction support for database operations\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"4. 🐛 Association explosion\")\n",
        "\n",
        "print(\"   Problem: Too many weak associations cluttering the system\")\n",
        "\n",
        "print(\"   Solution: Set minimum strength thresholds\")\n",
        "\n",
        "print(\"   Solution: Implement association decay and pruning\")\n",
        "\n",
        "print(\"   Solution: Limit association creation rules\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"5. 🐛 Context mismatch in retrieval\")\n",
        "\n",
        "print(\"   Problem: Retrieving irrelevant memories for current context\")\n",
        "\n",
        "print(\"   Solution: Improve context matching algorithms\")\n",
        "\n",
        "print(\"   Solution: Weight recent and relevant memories higher\")\n",
        "\n",
        "print(\"   Solution: Use semantic similarity for better matching\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"6. 🐛 Database corruption and persistence issues\")\n",
        "\n",
        "print(\"   Problem: SQLite database corruption or locking\")\n",
        "\n",
        "print(\"   Solution: Use proper connection management and timeouts\")\n",
        "\n",
        "print(\"   Solution: Implement backup and recovery mechanisms\")\n",
        "\n",
        "print(\"   Solution: Add data validation and integrity checks\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"7. 🐛 Memory type confusion\")\n",
        "\n",
        "print(\"   Problem: Wrong memory type assignment affecting retrieval\")\n",
        "\n",
        "print(\"   Solution: Clear guidelines for memory type classification\")\n",
        "\n",
        "print(\"   Solution: Automatic type inference based on content\")\n",
        "\n",
        "print(\"   Solution: Type validation and correction mechanisms\")\n",
        "\n",
        "print()\n",
        "\n",
        "print(\"🎉 Ready for Tutorial 11: Deliberative Reasoning!\")\n",
        "\n",
        "print(\"   Next we'll explore advanced decision-making capabilities...\")\n",
        "\n",
        "print(\"\\n🌟 You've now mastered:\")\n",
        "\n",
        "print(\"   • Individual agent intelligence and communication\")\n",
        "\n",
        "print(\"   • Multi-agent networks and collaborative processing\")\n",
        "\n",
        "print(\"   • Production monitoring, configuration, and management\")\n",
        "\n",
        "print(\"   • Structured agent circuits for complex workflows\")\n",
        "\n",
        "print(\"   • Sophisticated memory systems for context and learning\")\n",
        "\n",
        "print(\"\\n🚀 Ready to build truly intelligent agent systems!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJHuRHY4Auzy",
        "outputId": "f60876ee-cfdb-4027-8477-b766c44eb539"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  COMMON ERRORS AND SOLUTIONS:\n",
            "========================================\n",
            "1. 🐛 Memory leaks and unbounded growth\n",
            "   Problem: Memory systems growing without bounds\n",
            "   Solution: Implement proper decay mechanisms and cleanup\n",
            "   Solution: Set limits on memory system sizes\n",
            "   Solution: Use background maintenance threads\n",
            "\n",
            "2. 🐛 Slow memory retrieval performance\n",
            "   Problem: Linear search through large memory stores\n",
            "   Solution: Build proper indices (type, tag, context)\n",
            "   Solution: Use database storage with optimized queries\n",
            "   Solution: Cache frequently accessed memories\n",
            "\n",
            "3. 🐛 Memory consolidation conflicts\n",
            "   Problem: Race conditions during consolidation\n",
            "   Solution: Use proper threading and synchronization\n",
            "   Solution: Implement consolidation queues and batching\n",
            "   Solution: Add transaction support for database operations\n",
            "\n",
            "4. 🐛 Association explosion\n",
            "   Problem: Too many weak associations cluttering the system\n",
            "   Solution: Set minimum strength thresholds\n",
            "   Solution: Implement association decay and pruning\n",
            "   Solution: Limit association creation rules\n",
            "\n",
            "5. 🐛 Context mismatch in retrieval\n",
            "   Problem: Retrieving irrelevant memories for current context\n",
            "   Solution: Improve context matching algorithms\n",
            "   Solution: Weight recent and relevant memories higher\n",
            "   Solution: Use semantic similarity for better matching\n",
            "\n",
            "6. 🐛 Database corruption and persistence issues\n",
            "   Problem: SQLite database corruption or locking\n",
            "   Solution: Use proper connection management and timeouts\n",
            "   Solution: Implement backup and recovery mechanisms\n",
            "   Solution: Add data validation and integrity checks\n",
            "\n",
            "7. 🐛 Memory type confusion\n",
            "   Problem: Wrong memory type assignment affecting retrieval\n",
            "   Solution: Clear guidelines for memory type classification\n",
            "   Solution: Automatic type inference based on content\n",
            "   Solution: Type validation and correction mechanisms\n",
            "\n",
            "🎉 Ready for Tutorial 11: Deliberative Reasoning!\n",
            "   Next we'll explore advanced decision-making capabilities...\n",
            "\n",
            "🌟 You've now mastered:\n",
            "   • Individual agent intelligence and communication\n",
            "   • Multi-agent networks and collaborative processing\n",
            "   • Production monitoring, configuration, and management\n",
            "   • Structured agent circuits for complex workflows\n",
            "   • Sophisticated memory systems for context and learning\n",
            "\n",
            "🚀 Ready to build truly intelligent agent systems!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "🔒 **INTELLECTUAL PROPERTY & LICENSE NOTICE**\n",
        "\n",
        "This tutorial and its contents — including code, architecture, narrative examples, and educational structure — are the intellectual property of **Shalini Ananda, PhD** and part of the **Neuron Framework** under a **Modified MIT License with Attribution**.\n",
        "\n",
        "- Commercial use, redistribution, or derivative works **must** include clear and visible attribution to the original author.\n",
        "- Use in products, consulting engagements, or educational materials **must reference this repository and author name.**\n",
        "- Removal of author credit or misrepresentation of origin constitutes **a violation of the license and may trigger legal action.**\n",
        "- You may **not white-label, obfuscate, or rebrand** this work without explicit, written permission.\n",
        "\n",
        "Use of this tutorial in Colab or any other platform implies agreement with these terms.\n",
        "\n",
        "📘 **License**: [LICENSE.md](../LICENSE.md)  \n",
        "📌 **Notice**: [NOTICE.md](../NOTICE.md)  \n",
        "🧠 **Author**: [Shalini Ananda, PhD](https://github.com/ShaliniAnandaPhD)"
      ],
      "metadata": {
        "id": "WSNN26n7-WT9"
      }
    }
  ]
}